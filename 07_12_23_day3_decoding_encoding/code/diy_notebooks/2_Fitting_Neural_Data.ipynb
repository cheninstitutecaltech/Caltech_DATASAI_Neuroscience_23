{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "daf126a1",
   "metadata": {},
   "source": [
    "# Fitting a Linear Encoding Model to Miniscope Data\n",
    "\n",
    "Now that we've learned the basics of ridge regression, lets use it to build an encoding model with calcium imaging data. We will make use of scikit-learn, a popular machine learning library, to handle the regression fitting and crossvalidation.\n",
    "\n",
    "#### The dataset\n",
    "\n",
    "Neural activity was recorded from mouse anterior cingulate cortex (ACC) with a head-mounted miniscope. During the miniscope recording, the mouse performed a two-alternative forced choice (2AFC) task. The mouse had to discriminate between high-frequency (> 12 Hz) and low-frequency (< 12 Hz) auditory click trains. Depending on whether the stimulus was above or below this threshold, the mouse must poke its nose into the left or right spout to receive a water reward. The recording chamber was equipped with high speed cameras to track animal movements.\n",
    "\n",
    "Data was collected and preprocessed by Lukas Oesch in the Churchland Lab.\n",
    "\n",
    "#### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d99929",
   "metadata": {},
   "source": [
    " [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cheninstitutecaltech/Caltech_DATASAI_Neuroscience_23/blob/main/07_10_23_day1_ethics_regression/code/diy_notebooks/dimensionality_reduction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install gdown scikit-learn tqdm pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import encoding_tools\n",
    "\n",
    "data_path = encoding_tools.download_neural_data(\"miniscope\") #Download data\n",
    "design_matrix, Y_raw_fluorescence, neuron_footprints, frames_per_trial, frame_rate, aligned_segment_start = encoding_tools.load_miniscope(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a3bc3",
   "metadata": {},
   "source": [
    "### Show two example neurons in the field of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_tools.overlay_neurons(neuron_footprints, 56, 12, 12) #Pass the neuron ids for the red, green and blue channel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03b6a20d",
   "metadata": {},
   "source": [
    "#### Descriptions of the variables we loaded\n",
    "\n",
    "- **design_matrix**: This is the heart of the encoding model we are going to build. It contains a set of task and movement variables aligned to four distinct phases of the task, size is [number of total frames, regressor number]. It is equivalent to the matrix X from the previous notebook.\n",
    "\n",
    "- **Y_raw_fluorescence**: The fluorescence traces from all the neurons recorded aligned to the same task phases as the task- and movement variables, size is [number of total frames, number of neurons]\n",
    "\n",
    "- **neuron_footprints**: The spatial filters for the recorded neurons, size is [300 pixels, 300 pixels, neuron number].\n",
    "\n",
    "- **frames_per_trial**: The number of frames per trial.\n",
    "\n",
    "- **frame_rate**: The acquisition frame rate for the miniscope data.\n",
    "\n",
    "- **aligned_segment_start**: Marks the frames where the activity was aligned to a new task event. These also mark different task phases: Timepoints 0 to `aligned_segment_start[0]` contains the 1 second prior to trial initiation, the next phase is the 1 second from stimulus onset followed by action phase spanning from 200 ms before to 300 ms after movement onset and finally the outcome phase with 2 seconds after outcome presentation. The reason why the trial is split up into these phases is that there are variable delays before and after stimulus presentation and the choice report also happens at different speed.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a863e06",
   "metadata": {},
   "source": [
    "## Why build an encoding model?\n",
    "Ultimately, we would like to understand how different task variables (eg. stimulus, reward, and choice) and animal behaviors (eg. head orientation and DLC labels) affect neural activity. Ecoding models help us answer this by finding the relationship between movements and task variables ($X$) and neural activity ($Y$). There are many ways to do this, but we will chose to model a linear relationship between our predictor variables and the neural activity:\n",
    "\n",
    "$$\n",
    "Y = \\beta X\n",
    "$$\n",
    "\n",
    "As we did in the previous notebook, here we will solve for $\\beta$ using ridge regression. We will refer to the matrix $X$ as the *design matrix*. The design matrix is coded to contain all of the variables that we would like to predict neural activity with.\n",
    "\n",
    "Below is a figure from [Musall *et al* 2022](https://www.nature.com/articles/s41593-022-01245-9) that illustrates one possible design matrix. Animal movements (such as licking) and task variables (such as the stimulus) are coded into the design matrix. Other variables including pupil diameter, trial initiation tone, and hindlimb movements are also coded into the design matrix, but are not depicted here.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/design_matrix.PNG\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Within the design matrix, multiple time-shifted regressors for a particular variable are often included. This allows us to predict neural activity at a given time while making use of events that may have occurred before or after the exact moment in time we are trying to predict neural activity for. Here are some examples:\n",
    "- Stimulus related activity will appear with some delay, as shown in the bottom of the figure. So we want to allow the stimulus to affect neural activity after it comes on\n",
    "- Movements tend to have some \"preparatory\" activity that preceeds the actual execution of the movement. So we want a movement to be able to explain neural activity *before* it has occurred.\n",
    "\n",
    "**Coding the design matrix this way allows us to break free from the assumption that a particular event could only influence neural activity at the exact moment that it occurs**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63aa7fc8",
   "metadata": {},
   "source": [
    "## Exercise 1: The structure of the design matrix\n",
    "\n",
    "Lets start by looking at some of the task variables inside the design matrix:\n",
    "\n",
    "<ul>\n",
    "    <li>time (somewhat like an intercept term, explains fluctuations that happen every trial)</li>\n",
    "    <li>choice (left = 0, right = 1)</li>\n",
    "    <li>stimulus strength</li>\n",
    "    <li>outcome (incorrect = 0, correct = 1)</li>\n",
    "    <li>previous choice  (left = 0, right = 1)</li>\n",
    "    <li>previous outcome (incorrect = 0, correct = 1)</li>\n",
    "</ul> \n",
    "\n",
    "**Below, we will visualize part of the design matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS_TO_PLOT = 29 # starting from first trial\n",
    "N_REGRESSORS_TO_PLOT = 6 # starting from first regressor\n",
    "\n",
    "#Print some of the keys of the design matrix\n",
    "regs_2_plot = design_matrix.keys()[np.arange(0,N_REGRESSORS_TO_PLOT * frames_per_trial, frames_per_trial)].tolist()\n",
    "print(f'Plotting the following regressors: {regs_2_plot}')\n",
    "print(f'Full design matrix shape is {np.array(design_matrix).shape[0]} frames by {np.array(design_matrix.shape[1])} regressors.')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.array(design_matrix)[:N_TRIALS_TO_PLOT * frames_per_trial,:N_REGRESSORS_TO_PLOT * frames_per_trial], aspect='auto', cmap='seismic')\n",
    "#use np.array() on the design_matrix to be able to index numerically instead of by key\n",
    "for k in range(N_REGRESSORS_TO_PLOT):\n",
    "    plt.axvline(k * frames_per_trial, color='k', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "plt.colorbar()\n",
    "plt.xlabel('Regressor number')\n",
    "plt.ylabel('Time (frames)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c1fd91",
   "metadata": {},
   "source": [
    "#### Try to answer the following questions based on the plot above\n",
    "- What do the black dashed lines denote?\n",
    "- Why do we need many columns to represent one variable (such as choice)?\n",
    "- On the left side of the figure, why is the first column within the dashed lines full 1's on the diagonal?\n",
    "- Why are there blue/negative values in the third column?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e5a6c44",
   "metadata": {},
   "source": [
    "## Exercise 2: Estimating the regularization strength of the model\n",
    "\n",
    "As you have seen in the previous notebook, $\\alpha$ controls the shrinkage of the regressor weights and is critical to prevent overfitting. In this notebook we will use the `sklearn.linear_model.RidgeCV` class. RidgeCV searches through a grid of provided candidate $\\alpha$ values and determines the best one using *leave-one-out* cross-validation.\n",
    "\n",
    "In the following exercise you will be asked to:\n",
    "<ol>\n",
    "    <li> Choose appropriate parameters <strong>fit_intercept</strong> and <strong>alphas_per_target</strong> for RidgeCV.</li>\n",
    "    <li> Perform the grid search.</li>\n",
    "    <li> Retrieve the found regularization strengths as <strong>alphas</strong>.</li>\n",
    "</ol>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c625f048",
   "metadata": {},
   "source": [
    "### First, preprocess the data\n",
    "\n",
    "Here, we will set the inputs and outputs up for the following hyperparameter optimization and the model fitting and performance evaluation. We will need to z-score the response matrix, Y_raw_fluorescence, and the design_matrix as well. The latter one is extremely important to make sure that the different regressors are measured on the same scale, so all regressors get a fair chance to explain variance. However, here, we choose not to z-score the binary task variables because leaving them in their binary form preserves the interpretability of the model. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69742df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score the response matrix\n",
    "from scipy.stats import zscore\n",
    "Y = zscore(Y_raw_fluorescence, axis=0)\n",
    "\n",
    "#z-score the respective column of the design matrix\n",
    "scaler_obj = encoding_tools.standardize_x_cols(column_idx=np.arange(832, 1258)).fit(design_matrix) #Cols 0:827 are task vars, cols 828:832 are head orientation\n",
    "X = scaler_obj.transform(design_matrix)\n",
    "#Note: this class is defined in utils. It contains a fit and transform method, similar to StandardScaler from sklearn.preprocessing\n",
    "#We'll get back to this below\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120d141",
   "metadata": {},
   "source": [
    "Now, we will use `sklearn.linear_model.RidgeCV` to find the best values of $\\alpha$. You will need to pass the proper values for the following keyword arguments to `RidgeCV`:\n",
    "\n",
    "```\n",
    "alphas\n",
    "fit_intercept\n",
    "alpha_per_target\n",
    "scoring\n",
    "store_cv_values\n",
    "```\n",
    "You will likely want to look at [the sklearn RidgeCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) to determine what you should pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de205ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_test_range = 10**np.linspace(-10,9,50) #Get search values for alpha on log scale\n",
    "\n",
    "# 1. Define the RidgeCV paramters fit_intercept and alphas_per_target and explain why you chose them\n",
    "\n",
    "fit_intercept = #EXERCISE: pick the proper boolean\n",
    "alpha_per_target = #EXERCISE: pick the proper boolean\n",
    "\n",
    "# 2. Perform the grid search on the provided arrays X and Y. Use the RidgeCV function. SKLearn documentation will be helpful here.\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\"\"\"\n",
    "EXERCISE\n",
    "1. call RidgeCV with the proper arguments. Name this 'ridge_grid_search'\n",
    "ridge_grid_search = \n",
    "2. Call the .fit() method with the proper arguments\n",
    "3. Get the best alphas from the RidgeCV object. Store them as 'alphas'\n",
    "alphas = \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e0fb4",
   "metadata": {},
   "source": [
    "### Plot histogram of the best alpha values for the all of the neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e119b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.hist(alphas, bins=alpha_test_range)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Best alpha value')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc40a2fa",
   "metadata": {},
   "source": [
    "## Exercise 3: Assemble a pipeline for data preprocessing and model fitting with the found regularization strengths, alpha\n",
    "\n",
    "For this exercise you will use `sklearn.pipeline.Pipeline` ([docs here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)) to standardize the data, fit ridge models for the 10 splits of cross-validation, and evaluate the performance of each of them. \n",
    "\n",
    "- Create a `Pipeline` with two layers:\n",
    "1. Use `encoding_tools.standardize_x_cols` to z score columns of the design matrix. Name this layer of the pipeline `scaler`.\n",
    "2. Use `sklearn.linear_models.Ridge` to create the ridge regression model. Name this layer of the pipeline `ridge`.\n",
    "\n",
    "Store the pipeline with the name `pipe`.\n",
    "\n",
    "- Use the pipeline's `.fit()` method to fit the model on the training data, then `.predict()` to generate predictions with the testing data. We will use `sklearn.metrics.r2_score` to compute the $R^2$ on the held out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "N_FOLDS = 10\n",
    "X = np.array(design_matrix) #convert to array \n",
    "\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True) \n",
    "custom_scaler = encoding_tools.standardize_x_cols(column_idx = np.arange(832, 1258)) \n",
    "ridge_model = Ridge(alpha=alphas, fit_intercept=fit_intercept)\n",
    "\n",
    "#Set up the lists for beta weights and coefficient of determination of all the 10 models\n",
    "betas = []\n",
    "r_squared = []\n",
    "\n",
    "# 1. Create the pipeline for the fitting below. The pipeline should contain a 'scaler' step that calls standardize_x_cols\n",
    "#    and a ridge step that fits the model. Pipelines are nice because now the data standardization happens within each fold, which is best practice.\n",
    "\n",
    "pipe = #EXERCISE: create a Pipeline() with two layers as described above\n",
    "\n",
    "for train_index, test_index in kf.split(X, Y): #iterate over cv folds\n",
    "    # 2. Fit the training data sets using the pipleline and save the result as \"fits\". Then predict the unseen testing data\n",
    "    #    with the pipeline and store the results as \"predictions\"\n",
    "\n",
    "    fits = #EXERCISE: fit the pipeline with the training data\n",
    "    predictions = #EXERCISE: generate predictions from the pipeline with testing data\n",
    "\n",
    "    r_squared.append(r2_score(Y[test_index,:], predictions, multioutput = 'raw_values')) #score test performance\n",
    "    betas.append(pipe[-1].coef_)\n",
    "    \n",
    "betas = np.squeeze(betas)\n",
    "r_squared = np.squeeze(r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b82e7",
   "metadata": {},
   "source": [
    "### Plot the variance explained for each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_r2 = np.mean(r_squared, axis=0)\n",
    "idx = np.argsort(average_r2)[::-1]\n",
    "example_neuron_idx = [np.where(idx==56)[0][0], np.where(idx==12)[0][0]]\n",
    "\n",
    "ax = plt.figure().add_subplot(111)\n",
    "ax.plot(average_r2[idx], color = 'k')\n",
    "ax.scatter(example_neuron_idx, average_r2[[56,12]], color = 'g')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_bounds([0,0.8])\n",
    "ax.spines['bottom'].set_bounds([0,550])\n",
    "ax.set_xlabel('Neuron number')\n",
    "ax.set_ylabel('cross-validated R\\u00b2') \n",
    "ax.text(70, 0.66, 'Neuron 56 and neuron 12')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "605858d2",
   "metadata": {},
   "source": [
    "## Exercise 4: Plot the choice and previous choice encoding over the trial duration as estimated by the encoding model\n",
    "\n",
    "In addition to looking at the variance explained for each neuron, we can also plot the $\\beta$ weights to see how particular regressors influence neural activity.\n",
    "The aim of the final exercise of this notebook is to visualize the estimated weights for the previous choice and choice regressors across trial time.\n",
    "\n",
    "- Average the beta weights of the different fold to generate the mean weights. Name this `average_beta`.\n",
    "- Using what you've learned about the structure of the design matrix retrieve the beta weights for choice and previous choice over the trial duration\n",
    "- Visualize the beta weights for these regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start by getting the average beta weights\n",
    "\n",
    "average_beta = #EXERCISE: get the average beta weights across folds\n",
    "\n",
    "# 2. Retrieve the betas for time, choice and previous choice and name them choice_betas and previous_choice_betas. Make sure\n",
    "#    to build these varaibles so that the trial times are rows and the columns are neurons (you will probably need to transpose the matrix).\n",
    "\n",
    "time_betas = #EXERCISE: retrieve the betas for the time regressors\n",
    "choice_betas = #EXERCISE: retrieve the betas for the choice regressors\n",
    "previous_choice_betas = #EXERCISE: retrieve the betas for the previous choice regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First create a figure with four subplots\n",
    "axes_list = encoding_tools.create_subplot_axes(2, 2, fig_size=(9,9))\n",
    "\n",
    "#Now we can fill the subplots\n",
    "encoding_tools.plot_task_var_betas(choice_betas, time_betas, 'Choice', 56, aligned_segment_start, axes_list[0], colors = ['#8b8b8b', '#b20707'], var_value_strings = ['Intercept', 'Right choice'])\n",
    "encoding_tools.plot_task_var_betas(choice_betas, time_betas, 'Choice', 12, aligned_segment_start, axes_list[1], colors = ['#8b8b8b', '#b20707'], var_value_strings = ['Intercept', 'Right choice'])\n",
    "encoding_tools.plot_task_var_betas(previous_choice_betas, time_betas, 'Previous choice', 56, aligned_segment_start, axes_list[2], colors = ['#8b8b8b', '#ac3cbd'], var_value_strings = ['Intercept', 'Previous right choice'])\n",
    "encoding_tools.plot_task_var_betas(previous_choice_betas, time_betas, 'Previous choice', 12, aligned_segment_start, axes_list[3], colors = ['#8b8b8b', '#ac3cbd'], var_value_strings = ['Intercept', 'Previous right choice'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25522b77",
   "metadata": {},
   "source": [
    "### Examine the plots above and discuss the following questions\n",
    "- Why are the lines labeled Intercept represent?\n",
    "- What is the interpretation of these lines and how would you reconstruct a PETH for a right choice trial from the regressor weights?\n",
    "- Do these neurons mix encoding of different task variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b8220",
   "metadata": {},
   "source": [
    "## Sanity check: plotting PETH's\n",
    "\n",
    "To make sure our model is performing as expected, lets also plot some peri-event time histograms (PETH's). We will divide up trials by left vs right choice or left vs right previous choice. We will then plot the average response +/- SEM. Based on the plots above, our neurons seem to be encoding choice and previous choice, so we should see a nice separation in left vs right choice when plotting the PETH's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_frames = design_matrix.shape[0] #total number of frames in the session\n",
    "\n",
    "choice = design_matrix['choice_t0'][np.arange(0, n_total_frames, frames_per_trial)] \n",
    "previous_choice = design_matrix['previous_choice_t0'][np.arange(0, n_total_frames, frames_per_trial)]\n",
    "\n",
    "choice = np.array(choice)\n",
    "previous_choice = np.array(previous_choice)\n",
    "\n",
    "# Reshape the flourescence data matrix to obtain a timepoints x neuron id x trial matrix.\n",
    "n_neurons = Y_raw_fluorescence.shape[1]\n",
    "n_trials = int(design_matrix.shape[0] / frames_per_trial)\n",
    "Y_3d = np.reshape(Y_raw_fluorescence.T, (n_neurons, n_trials, frames_per_trial)).transpose(2,0,1)\n",
    "\n",
    "print(f'The shape of the calcium signal matrix was {Y_raw_fluorescence.shape}')\n",
    "print(f'The shape of the calcium signal matrix is now {Y_3d.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets call our function and plot the traces\n",
    "#First create a figure with four subplots\n",
    "axes_list = encoding_tools.create_subplot_axes(2, 2, fig_size=(10,10)) \n",
    "\n",
    "#Now we can fill the subplots\n",
    "encoding_tools.plot_mean_trace(Y_3d, choice, 'Choice', 56, aligned_segment_start, axes_list[0])\n",
    "encoding_tools.plot_mean_trace(Y_3d, choice, 'Choice', 12, aligned_segment_start, axes_list[1])\n",
    "encoding_tools.plot_mean_trace(Y_3d, previous_choice, 'Previous choice', 56, aligned_segment_start, axes_list[2], colors = ['#4fa97a', '#ac3cbd'])\n",
    "encoding_tools.plot_mean_trace(Y_3d, previous_choice, 'Previous choice', 12, aligned_segment_start, axes_list[3], colors = ['#4fa97a', '#ac3cbd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01acfa7",
   "metadata": {},
   "source": [
    "### Examine the plots and consider the following questions\n",
    "- How do these PETH plots compare to the plots of the $\\beta$ weights for choice and previous choice?\n",
    "- If there are differences, why do you think that is?\n",
    "\n",
    "In the next notebook, we will train the encoding model on shuffled versions of the design matrix. This will allow us to assess the variance that can be explained by individual task variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
