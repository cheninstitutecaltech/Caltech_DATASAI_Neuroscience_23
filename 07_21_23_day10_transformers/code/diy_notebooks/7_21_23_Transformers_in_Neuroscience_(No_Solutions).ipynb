{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwCar-cWry2t"
      },
      "source": [
        "# Transformers In Neuroscience\n",
        "\n",
        "### Author: Domenick Mifsud\n",
        "### Date: 5/21/23\n",
        "<br>\n",
        "\n",
        " [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cheninstitutecaltech/Caltech_DATASAI_Neuroscience_23/blob/main/07_21_23_day10_transformers/code/diy_notebooks/7_21_23_Transformers_in_Neuroscience_(No_Solutions).ipynb)\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers the following sections:\n",
        "\n",
        "1. [Setup](#scrollTo=Section_1_Setup)\n",
        "\n",
        "2. [What are Transformers?](#scrollTo=Section_2_What_are_Transformers_)\n",
        "    <br> 2.1&nbsp;&nbsp;[Background](#scrollTo=Section_2_What_are_Transformers_)\n",
        "    <br> 2.2&nbsp;&nbsp;[Self-Attention Overview](#scrollTo=Sec_2_2_Self_Attention_Overview)\n",
        "    <br> 2.3&nbsp;&nbsp;[Model Architecture](#scrollTo=Sec_2_3_Model_Architecture)\n",
        "    <br>\n",
        "3. [Applications to Neuroscience](#scrollTo=Section_3_Applications_to_Neuroscience)\n",
        "    <br> 3.1&nbsp;&nbsp;[fMRI](#scrollTo=Sec_3_1_fMRI)\n",
        "    <br> 3.2&nbsp;&nbsp;[Calcium](#scrollTo=Sec_3_2_Calcium)\n",
        "    <br> 3.3&nbsp;&nbsp;[Electrophysiology](#scrollTo=Sec_3_3_Electrophysiology)\n",
        "    <br>\n",
        "4. [Create a Neural Data Transformer](#scrollTo=Section_4_Create_a_Neural_Data_Transformer)\n",
        "    <br> 4.1&nbsp;&nbsp;[Feed Forward Layer](#scrollTo=Sec_4_1_Feed_Forward_Layer)\n",
        "    <br> 4.2&nbsp;&nbsp;[Multi-head Attention Layer](#scrollTo=Sec_4_2_Multi_head_Attention_Layer)\n",
        "    <br> 4.3&nbsp;&nbsp;[Encoder Layer](#scrollTo=Sec_4_3_Encoder_Layer)\n",
        "    <br> 4.4&nbsp;&nbsp;[The Full Model](#scrollTo=Sec_4_4_The_Full_Model)\n",
        "    <br>\n",
        "5. [Model Training & Evaluation](#scrollTo=Section_5_Model_Training_Evaluation)\n",
        "    <br> 5.1&nbsp;&nbsp;[Process the Data](#scrollTo=Sec_5_1_Process_the_Data)\n",
        "    <br> 5.2&nbsp;&nbsp;[Model Training](#scrollTo=Sec_5_2_Model_Training)\n",
        "    <br> 5.3&nbsp;&nbsp;[Model Evaluation](#scrollTo=Sec_5_3_Model_Evaluation)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXQGYe_bCbA3"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Section 1: Setup\n",
        "\n",
        "Before proceeding, please make sure that you are using a [GPU enabled runtime](https://stackoverflow.com/questions/50560395/how-to-install-cuda-in-google-colab-gpus/60338745#60338745) for this tutorial. This will take about 3 minutes, so feel free to start reading below after you press play!\n",
        "\n",
        "> **‚ö†Ô∏è&nbsp;&nbsp;WARNING: Only run this once!**\n",
        "\n",
        "\n",
        "\n",
        "&nbsp;‚¨á&nbsp; Press the play button below to download the data and install/import the required packages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiEzn_c7KhkF"
      },
      "source": [
        "#### Utils Package\n",
        "\n",
        "First, install a package from Github that contains some helper functions for this tutorial. Then, make sure that we are using a GPU enabled runtime before we proceed with the setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVvo-0kUqC3P"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/domenick-m/Caltech_DSAINSS_23.git\n",
        "import utils\n",
        "utils.assert_gpu_runtime()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoLLmiF7l1LQ"
      },
      "source": [
        "#### Install Packages\n",
        "\n",
        "Install required python packages using, `!pip install`. This command tells the operating system that is hosting the colab to install a package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA4RAJiPry2u"
      },
      "outputs": [],
      "source": [
        "!pip install ipywidgets\n",
        "!pip install jupyter\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install dandi\n",
        "!pip install git+https://github.com/neurallatents/nlb_tools.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYCEKJqEry2u"
      },
      "source": [
        "#### Download the Data\n",
        "\n",
        "We use a package called `dandi` to download the Neural Latents Benchmark data that they are hosting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy77CpyQry2u"
      },
      "outputs": [],
      "source": [
        "!dandi download https://dandiarchive.org/dandiset/000140"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGG8nuAHry2u"
      },
      "source": [
        "#### Package Imports\n",
        "Importing a package just allows us to use functions from it in this colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR5kM6Cgry2u"
      },
      "outputs": [],
      "source": [
        "import math # For basic math operations\n",
        "import torch  # For tensor operations\n",
        "import numpy as np  # For array operations\n",
        "import torch.nn as nn  # For neural network layers\n",
        "import torch.nn.functional as F  # For functional operations\n",
        "import matplotlib.pyplot as plt  # For plotting\n",
        "import plotly.graph_objects as go # For 3D plotting\n",
        "from nlb_tools.evaluation import evaluate\n",
        "from nlb_tools.nwb_interface import NWBDataset\n",
        "from nlb_tools.make_tensors import make_eval_target_tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfC06pHNry2v"
      },
      "source": [
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## Section 2: What are Transformers?\n",
        "\n",
        "### Sec 2.1: **Background**\n",
        "\n",
        "<p align = \"justify\">Transformer neural networks are sequence-to-sequence models, they take in a set of inputs and return a set of outputs (usually of the same length). Because they are such a general building block, there is no limit to the types of things that these models can do. For example, feature selection, generative modeling, denoising, and even reinforcement learning. They were originally designed for natural language processing (NLP), a subfield of artificial intelligence that focuses on enabling computers to understand, interpret, generate, and interact with human language in a meaningful way. Since then, they have not only been widely used in NLP, but have also shown promise in many other domains, such as computer vision, time series modeling, and even neuroscience.</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/transformer_inputs.png\" alt=\"inputs\" width=\"1100\"/>\n",
        "<br></br>\n",
        "\n",
        "<p align = \"justify\">Previously, neural network architectures have been specialized for the domain of interest. For example, with sequential data, Recurrent Neural Networks (RNNs) were often used, and for images, Convolutional Neural Networks (CNNs) were often used. However, in 2017 that all changed with the paper, \"<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>\" in which the original transformer was introduced. The original goal of the model was machine translation (translate a sentence from one language to another), but others found that this simple architecture can be applied to far more than just NLP. Let's now take a look at the architecture this model was designed to replace, the RNN.\n",
        "\n",
        "<p align = \"justify\">In an RNN, each input is given to the model independently and must follow the sequential ordering of the data, e.g. you must process <code>input_1</code> before <code>input_2</code>. In order for information from <code>input_1</code> to affect the output of <code>input_2</code>, we must have a way for the model to remember what was important from <code>input_1</code> and keep it in memory. We call this memory the \"<i>hidden state</i>\". We can only fit so much into the hidden state, so eventually you will need to forget some things.</p>\n",
        "\n",
        "<p align = \"justify\">One of the main advantages that transformers have over RNNs is that they do not have this sequential bottleneck, i.e. <b>information is not routed through a hidden state, but is directly exchanged between inputs</b>. This can be exteremly useful in situations where \"forgetting\" things about the previous inputs can be detrimental to the model's performance.</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/rnn_v_transformer.png\" alt=\"inputs\" width=\"800\"/>\n",
        "<br></br>\n",
        "\n",
        "<p align = \"justify\">In the NLP example above, we can see that when trying to predict the next word with the RNN, we only have information from the hidden state. With the transformer however, we can pull information from any words in the sequence! </p>\n",
        "\n",
        "<p align = \"justify\">This routing of information across the tokens is accomplished through the use of: <b><i>Self-Attention,</i></b> more specifically, <b><i>Scaled Dot Product Attention</i></b>. Let's now take a closer look at this \"attention\" mechanism and how it works...</p>\n",
        "<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8epocSfry2v"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Sec 2.2: **Self-Attention Overview**\n",
        "\n",
        "##### **Word Embeddings**\n",
        "\n",
        "<p align = \"justify\">To understand how attention can \"route\" information across inputs, we need to first understand the mechanism by which the inputs interact. In the example below, the inputs are words. To feed these words into a neural network we must first convert them into numbers so the network can process them. This is accomplished through the use of <b>word embeddings</b>.</p>\n",
        "\n",
        "<p align = \"justify\">Word embeddings are vectors that represent individual words in a multi-dimensional space, where the position of each word in the space reflects its semantic properties. Each dimension in the vector might represent how much the word relates to a different abstract concept. To illustrate this, let's take 3 example words:</p>\n",
        "\n",
        "* **`Fish`** üêü\n",
        "* **`Boat`** üö¢\n",
        "* **`Hunt`** üî´\n",
        "\n",
        "and represent them as 3D word embeddings. We will manually set the three dimensions to represent 3 arbitrary abstract concepts:\n",
        "\n",
        "1. Is it an **activity**?\n",
        "2. Is it closer related to the **water** or to **land**?   \n",
        "3. Is it related to **animals**?   \n",
        "\n",
        "Now, we must quantize our responses to those questions by forcing the answers to fall within a range of values (specific to each question). For this example we will use the same range for all dimensions, from -1 to +1.\n",
        "\n",
        "1. **Activity**: Closer to -1 means the word less often describes an activity, closer to +1 means the word often describes an activty\n",
        "\n",
        "1. **Water/Land**: Closer to -1 means the word is often closer related to water, closer to +1 means the word is often closer related to land, and zero means that it is often related the same amount to both concepts\n",
        "\n",
        "1. **Animals**: Closer to -1 means the word is less often related to animals, closer to +1 means the word is often realted to animals\n",
        "\n",
        "Let's now assign plausible values for the three dimensions of our example words: <i>(<b>NOTE:</b> while we are doing this manually, this process is often done through the use of a separate word embedding model like <a href=\"https://www.tensorflow.org/tutorials/text/word2vec\">Word2Vec</a>)</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk7WHwRory2v"
      },
      "outputs": [],
      "source": [
        "#      [activity,  sea/land,  animals]\n",
        "fish = [0.2,       -0.8,      0.9]\n",
        "boat = [-0.2,      -0.9,      -0.1]\n",
        "hunt = [0.8,       0.9,       0.8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q2IVhATry2v"
      },
      "source": [
        "Now that we have our 3 word embedding vectors, let's look at them in 3d space! (<b>HINT</b>: after running the following cell, click and drag to rotate the plot and scroll to zoom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Id8qaYAry2v"
      },
      "outputs": [],
      "source": [
        "data_tuples = [(fish, 'fish'), (boat, 'boat'), (hunt, 'hunt')]\n",
        "utils.create_3d_scatterplot(data_tuples, 'Activity', 'Water vs Land', 'Animals')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6YH95A8ry2w"
      },
      "source": [
        "We can visually see that boat and fish are closely related in this space!\n",
        "\n",
        "> A cool example (which allows us to look at different low-dimensional representations of high-dimensional embeddings) is this project: https://projector.tensorflow.org/\n",
        "\n",
        "##### **Why Dot Droduct?**\n",
        "\n",
        "Now let's take a look at the **dot product**, or how self-attention quantifies the similarity between these vectors. The dot product can be calculated as follows,\n",
        "\n",
        "$$ \\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_{i} b_{i} $$\n",
        "\n",
        "where $\\vec{a}$ and $\\vec{b}$ are n-dimensional vectors (in this case, n=3). Let's apply now check the similarity between our word embeddings using the NumPy (for matrix/vector operations) package to compute the dot product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9ov8L-mry2w"
      },
      "outputs": [],
      "source": [
        "utils.compute_similarity(data_tuples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "989jtc6Zry2w"
      },
      "source": [
        "Intuitively, this measure finds¬†fish¬†and¬†boat¬†to be closely related! It also determined that `fish` and `hunt` are more closely related than `boat` and `hunt`. Now it's your turn to create a word embedding!\n",
        "\n",
        "**Exercise:**\n",
        "\n",
        "Create your own word embedding vector and see find which of the example words it most closely relates with. Remember:\n",
        "- **Activity** (Dim 0): Closer to -1 means the word less often describes an activity, closer to +1 means the word is very often describes an activty\n",
        "\n",
        "- **Water/Land** (Dim 1): Closer to -1 means the word is often closer related to water, closer to +1 means the word is often closer related to land, and zero means that it is often related the same amount to both concepts\n",
        "\n",
        "- **Animals** (Dim 2): Closer to -1 means the word less often related to animals, closer to +1 means the word is very often realted to animals\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msIXwcTDImao"
      },
      "outputs": [],
      "source": [
        "# ----------------------------- Student Section ------------------------------ #\n",
        "# TODO: create a new word embedding and add it to the `data_tuples`\n",
        "# example_new_word = [activity,  sea/land,  animals]\n",
        "# data_tuples.append((example_new_word, 'example_new_word'))\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "# plot the embeddings in 3d space\n",
        "utils.create_3d_scatterplot(data_tuples, 'Activity', 'Water vs Land', 'Animals')\n",
        "# compute the similarity between all the tokens\n",
        "utils.compute_similarity(data_tuples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaSj4z2xIAVg"
      },
      "source": [
        "\n",
        "This section demonstrates that we can summarize higher-level concepts and information through embeddings, which we can use to efficiently compute similarity and measure these relationships concretely. This will be useful when we start thinking about how to use vectors and embeddings to pass information within the Transformer model.\n",
        "\n",
        "<br>\n",
        "\n",
        "##### **Queries, Keys, and Values**\n",
        "\n",
        "> ‚ö†Ô∏è&nbsp;&nbsp;<b>NOTE:</b> In this section we will refer to the word embeddings of the inputs as \"tokens\". While there are differences (see [here](https://neptune.ai/blog/tokenization-in-nlp)), it is easiest just to think of them as the same thing for now.\n",
        "\n",
        "Now, you may be wondering how attention actually performs any routing. Up until now, we have just seen the power of n-dimensional word embeddings and the ability of the dot product to give us a measure of how similar our embedding vectors are. The way in which these *tokens* prepare to exchange information is a 3 stage process that is performed independently for each token. Each token is asked 3 questions which it must answer using a linear combination of the dimsensions in its word embedding:\n",
        "\n",
        "- ***Question 1***: What information am I looking for in the other tokens?\n",
        "- ***Question 2***: What information do I have?\n",
        "- ***Question 3***: What information should I give to the other tokens?\n",
        "\n",
        "Those 3 questions are also referred to as the queries, the keys, and the values:\n",
        "\n",
        "1. ***Queries***: what information am I looking for in the other tokens?\n",
        "2. ***Keys***: what relevant information do I have?\n",
        "3. ***Values***: what information should I give to the other tokens?\n",
        "<br>\n",
        "\n",
        "As an example, let's illustrate what a token-to-token interaction via self attention might look like for the sentence:\n",
        "\n",
        "> `The boy ran fast`\n",
        "\n",
        "from the point of view of the `fast` token (randomly chosen example).\n",
        "\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/attention_3_qs.png\" alt=\"inputs\" width=\"1100\"/>\n",
        "<br>\n",
        "\n",
        "> <p align = \"justify\">Here we can see some examples of the types of \"answers\" that a token might reply to the 3 questions posed earlier with (remember that the replies given by the tokens are just linear combinations of their word embeddings). We are only going to look at the query-key interactions for the <code>fast</code> token, but the same interactions will be happening for all the queries in the sentence in parallel (all at once).</p>\n",
        "\n",
        "Let's now look at the math behind this step to see exactly how this is done. First, we will define the following vectors:\n",
        "\n",
        "$$ \\vec{x_1} \\,\\text{ is the word embedding of \"The\"} $$\n",
        "$$ \\vec{x_2} \\,\\text{ is the word embedding of \"boy\"} $$\n",
        "$$ \\vec{x_3} \\,\\text{ is the word embedding of \"ran\"} $$\n",
        "$$ \\vec{x_4} \\,\\text{ is the word embedding of \"fast\"} $$\n",
        "\n",
        "The queries, keys, and values for the <code>fast</code> token would be mathematically represented as:\n",
        "\n",
        "$$ \\vec{q_{\\,x_4}} = \\vec{x_4} W_Q$$\n",
        "$$ \\vec{k_{x_4}} = \\vec{x_4} W_K$$\n",
        "$$ \\vec{v_{x_4}} = \\vec{x_4} W_V$$\n",
        "\n",
        "where $\\vec{q_{\\,x_4}}$, $\\vec{k_{\\,x_4}}$, and $\\vec{v_{\\,x_4}}$  are the query, key, and values of the <code>fast</code> tokens respectively and $W_Q$, $W_K$, $W_V$ are the weight matrices for the queries, keys, and values respectively. While this is only the math for the <code>fast</code> token, the same equations (apart from swapping $\\vec{x_4}$ to the correct vector) are applied to all the tokens.\n",
        "\n",
        "<br>\n",
        "<b>Filing Cabinet Analogy</b>\n",
        "\n",
        "To make the interactions between the tokens easier to understand, let's imagine a conceptual example in which each token has a folder in a filing cabinet.\n",
        "- The <b>queries</b> represent what information we want to look for in the filing cabinet to update each token.\n",
        "- The <b>keys</b> represent the labels on the folders, indicating what is inside them.\n",
        "- The <b>values</b> represent the actual contents of the folders, the files of interest.\n",
        "<br>\n",
        "\n",
        "\n",
        "We will be using the queries, keys, and values (the \"answers\" to the 3 questions we posed earlier) from the last example.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/attention_folders_1.png\" alt=\"inputs\" width=\"1100\"/>\n",
        "\n",
        "> Here we can see the folders, the keys (labels that indicate what is in each folder), and the values (the files of interest from within the folders) for each token in the sentence. We can also see the query for the <code>fast</code> token that it will use to search for information to update itself with.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/attention_folders_2.png\" alt=\"inputs\" width=\"1100\"/>\n",
        "<br>\n",
        "\n",
        "> <p align = \"justify\">Here we can see the analogous process of calculating the attention scores. We are comparing the query of the <code>fast</code> token to keys of all the other tokens and getting a value to determine how close the content within the folders (key) matches what we are looking for (query). This could be considered a method of assigning importance or determining the proportion of papers we should retrieve from other folders to update our <code>fast</code> folder. The <i>self</i> in self-attention refers to the fact that, when determining how to update the¬†fast¬†folders contents, we are also able to use the¬†fast¬†folders existing contents in addition to the contents of the other folders.</p>\n",
        ">\n",
        "> In this example we are pulling information from mainly the action that is happening, `ran`, becuase that is very likely to be the action that we, as the adverb, are describing. However, we also pull information from descriptors of the verb such as some information about the subject performing the action, and some info about the how the action is being performed. This extra information gives more context to the action!\n",
        "\n",
        "To calculate these attention scores for the <code>fast</code> token ($\\vec{x_4}$), we use the dot product of the query and key vectors. Let's denote $z_{i}$ as the unnormalized score for a token i, which is calculated as:\n",
        "\n",
        "$$z_{i} = \\frac{\\vec{q_{\\,x_4}} \\cdot \\vec{k_i}}{\\sqrt{d_k}}$$\n",
        "\n",
        "Where $d_k$ is the dimensionality of $\\vec{k_i}$. We can then apply the softmax function to calculate the probabilities:\n",
        "\n",
        "$$\\sigma(z_{i}) = \\frac{e^{z_{i}}}{\\sum_{j \\in \\{x_1,\\,x_2,\\,x_3,\\,x_4\\}} e^{z_{j}}} $$\n",
        "\n",
        "\n",
        "This gives us the attention weights ($a_i$) for each token (i) in the sentence:\n",
        "\n",
        "$$a_{x_1} = \\sigma(z_{x_1})$$\n",
        "$$a_{x_2} = \\sigma(z_{x_2})$$\n",
        "$$a_{x_3} = \\sigma(z_{x_3})$$\n",
        "$$a_{x_4} = \\sigma(z_{x_4})$$\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/attention_folders_3.png\" alt=\"inputs\" width=\"1100\"/>\n",
        "<br>\n",
        "\n",
        "> <p align = \"justify\"> The attention scores that we just calculated will then be used used to weight the values (the files of interest from each folder) and combine them to form the updated representation. The folder that we are updating (<code>fast</code>) only has a limited amount of space for papers, so we cannot fit <i>all</i> the papers that we pulled. Hence the importance of the weighting procedure, we need to carefully choose from which folder we want to use the papers from.</p>\n",
        "\n",
        "For the last of the math behind the attention calculation, we can use the attention scores ($a_{x_1}$, $a_{x_2}$, $a_{x_3}$, $a_{x_4}$) to compute a weighted sum of the value vectors to get the new representation for the `fast` token:\n",
        "\n",
        "$$\\text{Updated Representation}(\\vec{x_4}) = a_{x_1}\\vec{v_{x_1}} + a_{x_2}\\vec{v_{x_2}} + a_{x_3}\\vec{v_{x_3}} + a_{x_4}\\vec{v_{x_4}}$$\n",
        "<br>\n",
        "\n",
        "##### <b>Multi-Head Attention</b>\n",
        "<p align = \"justify\">To extend the filing cabinet analogy to <i>multi-head</i> attention, think of us having a list of questions (queries) that we are asking to update each folder. And now we also have a list of labels (keys) on each of the files that correspond to the those questions.</p>\n",
        "\n",
        "<p align = \"justify\">For each question in our list, we will pull a number of papers. Now we can do this in two ways, the first, by only pulling a limited amount of papers for each question (so that each question can put information into the new folder). Or alternatively, we can pull more papers for each question than we can fit into the new folder (and just sort through them and pull out the important info from each). These are the differences in the two ways in which we pull together the information from across the heads, either through strictly concatenation, or with the additional use of a linear projection (which places no restrictions on the dimensions of the queries, keys, and values).</p>\n",
        "\n",
        "<p align = \"justify\">Why is multihead attention even neccesary? Because there are no interactions between the queries and the values directly, i.e. the papers that are pulled from each folder will be the same regardless of what we are looking for. That's why we must have multiple routings going on at once so that we can get different types of information from the folders.</p>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Gf-zgO5AVd"
      },
      "source": [
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QdwBCVfZYIt"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Sec 2.3: **Model Architecture**\n",
        "The original transformer, as explained in the paper, \"[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\" employs the encoder-decoder architecture.\n",
        "\n",
        "Before diving into training and inference examples with the model, we need to understand what the original transformer was built to accomplish: machine translation. This implies that the model is given a sentence in one language and it has to generate that same sentence in a different language.\n",
        "\n",
        "In the figures below, the input embeddings refer to the word embeddings of the original sentence, which we discussed in the last section. The output embeddings, on the other hand, pertain to the word embeddings of the words in the target language, the one to which we want to translate.\n",
        "\n",
        "In Natural Language Processing (NLP), special tokens are often used to denote the beginning and end of a sentence. These tokens are represented by \"\\<sos>\" (start of sentence) and \"\\<eos>\" (end of sentence). We append the \"\\<sos>\" token to the start of the output embedding so that the translated sentence is \"lagged\" behind the source sentence.\n",
        "\n",
        "Here is a brief overview of the architecture of the transformer, we will dive into each part in more detail in the later sections:\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/Transformer.png\" alt=\"inputs\" width=\"1100\"/>\n",
        "\n",
        "> The full transformer model, the input embedding being the words to translate and the output embedding being the words you've translated so far.\n",
        ">\n",
        "> a.) The Transformer Encoder, takes in the source sentence (needs to be translated) and gives context (as we saw in the last section) to the word embeddings or tokens.\n",
        ">\n",
        ">b.) The Transformer Decoder, this takes in the contextualized source tokens and what we have translated so far and outputs the next word in the translation. The Nx means that the encoder and the decoder blocks will be stacks of N layers (meaning the same functions will be applied over and over again, for N times).\n",
        "\n",
        "\n",
        "Now we will can look in detail at the *training* and *inference* processes of the model. Training is the process by which we update the trainable parameters of the model and inference is the process in which we pass data through the model for evaluation or real-time use.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/encoder-decoder-infer.png\" alt=\"inputs\" width=\"800\"/>\n",
        "\n",
        "> Let's first take a look at inference with an example one layer transformer. We can see it requires multiple passes into the decoder to generate the output because it only outputs one token into the future. We must then take that output and feed it back into the decoder again, this process repeats until the \"\\<eos>\" token is predicted. As you can probably see, this is quite similar to an RNN.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/encoder-decoder-train.png\" alt=\"inputs\" width=\"800\"/>\n",
        "\n",
        "> Now here is the training procedure. Notice that instead of feeding in one word at a time like is done during inference, we just give it the entire translated sentence. This is much faster as we only need to do one \"forward pass\" or run the data through the model once and much more memory efficient as we dont need to store each progressive step into memory for the \"backward pass\" (when we are actually updating the weights of the model).\n",
        "\n",
        "Quite a difference between the inference and the training procedures, huh? Let's break down those differences and try and understand the power of parallel training.\n",
        "\n",
        "- **Teacher Forcing**: Notice how when running inference with the model, we have to recursively feed the next predicted word into the decoder again, but when training we just feed the whole thing in at once? This is called, *Teacher Forcing*, and is a technique that helps the model train faster by limiting the effects that an incorrect prediction will have on the future. Because of the sequential dependences of the RNN, a mistake early on will propagate throughout the entire sequence. However with teacher forcing, the model is only tasked with predicting the next word not the entire sequence again!\n",
        "\n",
        "- **Masked Self-Attention**:\n",
        "Inside the decoder, masked multi-head attention module speeds up training by treating each output embedding as a distinct training instance. This is a departure from the traditional method where the entire sequence history must be input for each sample. Instead, we just do this once! It's a quicker, smoother process.\n",
        "\n",
        "    - Now, you may be wondering about the optimal output of these \"training instances\". If each token's optimal output is simply the next word (the token immediately to its right), and if the MHA architecture allows for the flow of information across tokens, could the next token's word simply be \"pulled in\"? This is where the beauty of the masking feature comes in. Despite the interconnection of tokens within the MHA, the 'masked' part ensures that a token is 'blind' to what comes next. In essence, each token has access only to the tokens that precede it, its 'past', so to speak. This keeps the integrity of the learning process intact and ensures that every token effectively learns its place in the sequence.\n",
        "\n",
        "\n",
        "Let's break down a few of the architectural features that have not yet been covered:\n",
        "\n",
        "1. **Linear & Softmax**: The final output of the decoder is then passed through a linear layer and a softmax function. This is to get the probability of what the next word will be.\n",
        "\n",
        "2. **Feed Forward**: This is just a sequence of: linear layer, activation function, and then another linear layer. This crucial step will be discussed more in the later sections.\n",
        "\n",
        "3. **Positional Encoding**: a way to indicate the position of words in a sentence. This is because the transformer architecture operates on *sets*, this means that it is unphased by a shuffling of the input tokens and will always return the same result. Positional encoding is a way to indicate the position of words in a sentence. It's based on sine and cosine functions which generate a unique but consistent positional signal.\n",
        "\n",
        "    The Positional Encoding for position 'pos' and dimension 'i' is calculated as:\n",
        "\n",
        "    - If 'i' is even: sin(pos / 10000^(i/d_model))\n",
        "    - If 'i' is odd: cos(pos / 10000^(i/d_model))\n",
        "\n",
        "    This can be represented as:\n",
        "\n",
        "$$PE(pos,2i) = sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)$$\n",
        "$$PE(pos,2i+1) = cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)$$\n",
        "\n",
        "We will now implement the positional encoding a bit differently (as is done in the real *Attention Is All You Need* code) in PyTorch.\n",
        "\n",
        "**Exercise:**\n",
        "\n",
        "Implement the formula for positional encoding in Python, and generate a heat map of positional encodings for sequence of 50 positions and 16 dimensions. Variables you need to complete:\n",
        "\n",
        "1. `angle_rates`: Use the [`torch.pow()`](https://pytorch.org/docs/stable/generated/torch.pow.html) function to raise `10000` to the power of ( `depths` divided by `float(half_dimensions)`). This will create a tensor of angle rates. Then, divide `1` by this tensor to get the inverse, which gives the final angle rates.\n",
        "\n",
        "2. `angle_radians`: Multiply `positions` by `angle_rates`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P0tj3XO5GqG"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(sequence_length, dimensions):\n",
        "    # Half the dimensions since sine and cosine will duplicate it\n",
        "    half_dimensions = dimensions // 2\n",
        "\n",
        "    # Generate positions and depths\n",
        "    positions = torch.arange(sequence_length).unsqueeze(1)   # Shape: (sequence_length, 1)\n",
        "    depths = torch.arange(half_dimensions).unsqueeze(0)      # Shape: (1, half_dimensions)\n",
        "\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "    # TODO: Implement `angle_rates`\n",
        "    # TODO: Implement `angle_radians`\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "    # Apply sine and cosine, and concatenate results\n",
        "    encoded_positions = torch.cat(\n",
        "        [torch.sin(angle_radians), torch.cos(angle_radians)], dim=-1)    # Shape: (sequence_length, dimensions)\n",
        "\n",
        "    return encoded_positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_NSnJwY801t"
      },
      "source": [
        "Let's now look at the different vectors it creates for all of the 50 positions we supply it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v31jPZsHEVq6"
      },
      "outputs": [],
      "source": [
        "pos_encoding = positional_encoding(sequence_length=50, dimensions=16)\n",
        "\n",
        "plt.pcolormesh(pos_encoding.T, cmap='RdBu')\n",
        "plt.ylabel('Dimensions')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvo0dSPPEwD4"
      },
      "source": [
        "We can see that the top 8 dimensions are the `sin()` values and the bottom 8 values are the `cos()` values. Each of the 8 dimensions has a different frequency which allows the model to look for other tokens using multiple scales of distance.\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDGIyYniyEhf"
      },
      "source": [
        "<br>\n",
        "\n",
        "## **Section 3: Applications to Neuroscience**\n",
        "<p align = \"justify\">Now that we have an understanding of how attention works, let's take a look at some specific examples of how transformers (and the concepts of self attention) have been applied to neuroscientific research. These examples are across 3 example data modalities, as one of the best ways to understand a new topic is to connect it with what you already know!</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weaFpbvpTMQv"
      },
      "source": [
        "\n",
        "### Sec 3.1: **fMRI**\n",
        "\n",
        "#### [Semantic reconstruction of continuous language from non-invasive brain recordings](https://www.biorxiv.org/content/10.1101/2022.09.29.509744v1.full.pdf) (2022)\n",
        "\n",
        "*Jerry Tang, Amanda LeBel, Shailee Jain, Alexander G. Huth*\n",
        "\n",
        "<p align = \"justify\">This paper presents a novel brain-computer interface (BCI) that uses functional magnetic resonance imaging (fMRI) to decode continuous language. Unlike traditional BCIs, this new model doesn't require invasive procedures or limit itself to a pre-defined set of words. Instead, it generates a sequence of words that closely matches what the subject is hearing, imagining, or observing in silent videos. This is achieved by using a neural network model, specifically a Transformer model, which is a type of deep learning model well-suited for processing sequences of data.</p>\n",
        "\n",
        "<p align = \"justify\">The researchers trained this model by having subjects listen to hours of spoken narrative stories while recording their brain activity with fMRI. The model learned to predict the fMRI response to different phrases in these stories. When decoding, the model generates candidate word sequences and scores them based on how likely they are to have evoked the recorded brain responses. The best sequence is then selected as the output. This approach allows the model to overcome the low temporal resolution of fMRI, which makes it challenging to link specific words with their corresponding brain images.</p>\n",
        "\n",
        "<p align = \"justify\">The paper also highlights that the model can successfully decode language from different cortical networks, suggesting that these networks contain redundant information about language processing. The authors also demonstrate the practical applications of the model, such as reconstructing imagined speech and silent films, and show that the model can be consciously resisted, which has important privacy implications. Despite the complex deep learning techniques involved, the takeaway is that this approach has potential to develop non-invasive BCIs that could significantly aid communication for those with speech and language disorders.</p>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/huth_fmri_r.png\" alt=\"inputs\" width=\"1100\"/>\n",
        "\n",
        "> a.) *BOLD fMRI responses were recorded while three subjects listened to 16 h of narrative stories. An encoding model was estimated for each subject to predict brain responses from semantic features of stimulus words.* b.) *To reconstruct language from novel brain recordings, the decoder maintains a set of candidate word sequences. When new words are detected, a language model (LM) proposes continuations for each sequence and the encoding model scores the likelihood of the recorded brain responses under each continuation. The most likely continuations are retained.*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/huth_decoded_words_r.png\" alt=\"inputs\" width=\"900\"/>\n",
        "\n",
        "> c.) *Decoders were evaluated on single-trial brain responses recorded while 5 subjects listened to test stories that were not used for model training. Segments from four test stories are shown alongside decoder predictions for one subject. The decoder exactly reproduces some words and phrases, and captures the gist of many more.*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uh-yMhmx_so"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Sec 3.2: **Calcium**\n",
        "\n",
        "#### [Spatial redundancy transformer for self-supervised fluorescence image denoising](https://www.biorxiv.org/content/biorxiv/early/2023/06/05/2023.06.01.543361.full.pdf) (2023)\n",
        "\n",
        "By: *Xinyang Li, Xiaowan Hu, Xingye Chen, Jiaqi Fan, Zhifeng Zhao, Jiamin\n",
        "Wu, Haoqian Wang, & Qionghai Dai*\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align = \"justify\">This paper presents a new technique called the Spatial Redundancy Denoising Transformer (SRDTrans) to improve the quality of fluorescence microscopy images. Fluorescence microscopy, including techniques like calcium imaging, can be negatively affected by factors such as labeling concentration, brightness of the dye, and photobleaching. The noise introduced by these factors can make the resulting images difficult to interpret.</p>\n",
        "\n",
        "<p align = \"justify\">The SRDTrans method is a self-supervised technique that improves image quality by reducing noise. Unlike previous methods, it doesn't rely on the similarity between consecutive frames but instead uses a \"spatial redundancy sampling strategy.\" This means it uses data from the original image in two different orientations to train itself. SRDTrans is based on a network architecture known as a transformer, which unlike traditional methods, can maintain high resolution while using fewer computational resources.</p>\n",
        "\n",
        "<p align = \"justify\">Applied to calcium imaging, the SRDTrans method was able to effectively reduce noise and recover structures that were previously hard to distinguish, such as soma, neurites, and vascular shadows. Importantly, it was also able to restore most of the frequency components that other methods lose, leading to better denoising performance. This indicates that SRDTrans could be a powerful tool for improving the quality of calcium imaging data, and potentially other fluorescence microscopy techniques as well.</p>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/SRDTrans_a_r.png\" alt=\"inputs\" width=\"1100\"/>\n",
        "\n",
        "> <p align = \"justify\">a.) <i>Self-supervised training strategy of SRDTrans. In a single instance of spatial redundancy sampling, the original low-SNR stack of H√óW√óT pixels is sampled by orthogonal masks, producing three downsampled sub-stacks (input, target 1, and target 2) of H/2√óW/2√óT pixels. The ‚Äúinput‚Äù sub-stack is fed into the transformer network, and the corresponding output is compared with the ‚Äútarget‚Äù sub-stacks to calculate the loss function for parameter optimization.</i></p>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/SRDTrans_c_r.png\" alt=\"inputs\" width=\"900\"/>\n",
        "\n",
        "> <p align = \"justify\">c.) <i>The architecture of the lightweight spatiotemporal transformer. It consists of a temporal encoder module, a spatiotemporal transformer block (STB), and a temporal decoder module. Each temporal encoder compresses the temporal scale (t) of the input by a factor of r (adjustable and set as 4 in this work) using convolution. In the STB module, the input is divided into small patches, and different feature maps of the same spatial position are stitched together in the patch flattening layer. The position embedding layer records the spatial position of each patch so that it can be mapped back after the global interaction in the multi-head self-attention layer. The self-attention mechanism can calculate the spatiotemporal correlation between all local patches. Then the output patches are remapped to their original spatial positions. The output of the STB module will be uncompressed to the original temporal scale by the following temporal decoder module. Multiple skip connections are configured to stabilize the training of the network.</i></p>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/SRDTrans_e_r.png\" alt=\"inputs\" width=\"900\"/>\n",
        "\n",
        "> <p align = \"justify\">e.) <i>Comparing the denoising performance of SRDCNN and SRDTrans on simulated calcium imaging data (30 Hz). Magnified views of boxed regions are shown at the top left of the images. Purple arrowheads point to a dendritic fiber. Scale bar, 40‚ÄâŒºm for the whole field of view (FOV) and 10‚ÄâŒºm for magnified views.</i></p>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mszyhViycxp"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Sec 3.3: **Electrophysiology**\n",
        "\n",
        "#### [Representation learning for neural population activity with Neural Data Transformers](https://arxiv.org/pdf/2108.01210.pdf) (2021)\n",
        "\n",
        "\n",
        "\n",
        "By: *Joel Ye & Chethan Pandarinath*\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align = \"justify\">On the electrophysiology side of things, the Neural Data Transformer (NDT) uses an encoder only transformer architecture like <a href=\"https://arxiv.org/abs/1810.04805\">BERT</a> that takes in multi-electrode spiking activity, or simultaneously recorded action potentials (\"spikes\") from multiple neurons. NDT is does not have an explicit dynamics model like similar RNN-based models for analyzing spiking activity such as <a href=\"https://arxiv.org/pdf/1608.06315.pdf\">LFADS</a>, information can flow throughout time anyway it pleases.</p>\n",
        "\n",
        "<p align = \"justify\"> The authors argue that while Recurrent Neural Networks have been successfully used to model the dynamics of such neural spiking activities, Transformers could offer advantages such as faster training and processing speeds, particularly beneficial for real-time applications like brain-computer interfaces.</p>\n",
        "\n",
        "<p align = \"justify\"> The NDT model is designed with unique attributes to adapt to the specific nature of spiking activity, which is distinct from the data that language transformers and other time series models usually handle. NDT is used to transform sequences of binned spiking activity into inferred firing rates. The transformer model, unlike the sequential nature of LFADS, processes all inputs together in layers, allowing it to avoid sequential bottlenecks. This paper details the training process for the NDT model, emphasizing the need for careful training decisions due to the typically small size of neuroscientific datasets.</p>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/NDT.png\" alt=\"inputs\" width=\"900\"/>\n",
        "<ol type=\"a\">\n",
        "  <li>The full architecture of the NDT model that you will be creating today.</li>\n",
        "  <li>The input matrix, a multi-channel iBCI spike train. </li>\n",
        "  <li>While NDT is based off of BERT, which uses no masking in the attention, NDT uses maked attention to limit how far forwards and backwards into time each token can attend to. The standard NDT model uses 2 attention heads.</li>\n",
        "  <li>The output matrix, inferred firing rates.</li>\n",
        "</ol>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMTnJmkGry2w"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Section 4: Create a Neural Data Transformer\n",
        "\n",
        "In this section we will be creating a simplified version of the Neural Data Transformer (previously discussed in [Sec. 3.3](#scrollTo=1mszyhViycxp&line=26&uniqifier=1)).\n",
        "\n",
        "##### **Introductory Python / ML Terms:**\n",
        "\n",
        "* <p align = \"justify\"><a href=\"https://uwpce-pythoncert.github.io/ProgrammingInPython/modules/SubclassingAndInheritance.html\"><b><i>Subclass</b></i></a>: An object that gets to use all the functions defined in its \"superclass\". By creating a subclass you are saying: this Object is a <i>superclass</i>, but heres even more things that it can do that <i>superclass</i> can't.</p>\n",
        "\n",
        "* <p align = \"justify\"><a href=\"https://www.linkedin.com/advice/3/what-benefits-drawbacks-batch-processing-ml-skills-batch-processing\"><b><i>Batch</b></i></a>: A bunch of inputs being processed in parallel (at the same time) to speed up neural network training. Think of it like like an airport shuttle, and all the people need to make the same trip, it makes sense to load the shuttle with as many people as you can fit!</p>\n",
        "\n",
        "* <p align = \"justify\"><a href=\"https://pytorch.org/docs/stable/nn.html#linear-layers\"><b><i>Linear Layer</b></i></a>: A.K.A. a Fully-Connected Layer, is a matrix multiplication on the incoming data with a learned \"weight\" matrix and then addition with a learned \"bias\" term, or: $y = Wx + b$</p>\n",
        "\n",
        "* <p align = \"justify\"><a href=\"https://pytorch.org/docs/stable/nn.html#normalization-layers\"><t></t><b><i>Normalization</b></i></a>: A technique that standardizes or scales input data in order to prevent the model from being biased towards features with higher magnitudes. In a sense, it's a way to make sure that each input feature has the same level of importance for the learning algorithm.</p>\n",
        "\n",
        "  - <p align = \"justify\">Transformers often use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\"><b>Layer Normalization</b></a> (LayerNorm), where the mean and variance are computed independently for each input across all its features, adjusting and scaling the activations within the current layer. This is different from other normalization techniques such as Batch Normalization, which computes a single mean and variance for the entire batch.</p>\n",
        "\n",
        "\n",
        "    \n",
        "* <p align = \"justify\"><a href=\"https://pytorch.org/docs/stable/nn.html#dropout-layers\"><t></t><b><i>Dropout</b></i></a>: A regularization technique where a proportion of the features in the inputs are randomly \"dropped out\" or set to zero during training. This prevents the model from overly relying on the exact presentation of any given feature, forcing the model to develop more robust representations.</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlHYDUI7ry2w"
      },
      "source": [
        "### Sec 4.1: **Feed Forward Layer**\n",
        "\n",
        "<p align = \"justify\">The <a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\">Feed-Forward Network</a> (FFN) is a crucial part of the transformer architecture. It is applied independently to each token in the sequence, so this operation is highly parrallelizable (speeds up training and execution of the model because you can do it all tokens in one operation). The FFN consists of two linear layers and a non-linearity in between. It is essential for introducing non-linearity to the system and allowing the model to learn more complex representations. Despite its name, it doesn't carry information forward in terms of sequential data but instead contributes to the transformation of the input data within each individual layer.</p>\n",
        "\n",
        "* Transformers often use the [rectified linear unit](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) (ReLU) as their choice of non-linearity, which just sets negative values to 0. Simple huh!\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/FFN.png\" alt=\"inputs\" width=\"900\"/>\n",
        "<br></br>\n",
        "\n",
        "##### **Exercise**\n",
        "Now let's implement the FFN using PyTorch. You will need to create a class called FeedForwardNetwork, which is a subclass of nn.Module.\n",
        "\n",
        "**You need to provide the class with code to do two things:**\n",
        "1. First to initialize the various PyTorch layers/functions to the requested sizes within the `__init__()` method of your class.\n",
        "2. Then, to transform the incoming data using those layers/functions within the `forward()` method of your class.\n",
        "\n",
        "**The FFN class should have the following parameters:**\n",
        "- `self.norm` ([`nn.LayerNorm`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#layernorm)): normalizes data of shape `input_dim`\n",
        "- `self.linear1` ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)): transforms the input dimensions from `input_dim` to `hidden_dim`\n",
        "- `self.relu` ([`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)): activation function\n",
        "- `self.dropout` ([`nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout)): zeros out features with a `dropout_p` probability\n",
        "- `self.linear2` ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)): transforms the input dimensions from `hidden_dim` to `input_dim`\n",
        "\n",
        "> **‚ö†Ô∏è&nbsp;&nbsp;WARNING: Make sure you use the variable names defined above!**\n",
        "\n",
        "\n",
        "Next, within the `forward()` method, you are to define the forward pass for this network.\n",
        "<br>\n",
        "**This method should:**\n",
        "1. Pass the input `x` through the LayerNorm\n",
        "2. Pass the output of (1) through the first linear layer\n",
        "3. Pass the output of (2) through the ReLU activation\n",
        "4. Pass the output of (3) through the dropout layer\n",
        "5. Pass the output of (4) through the second linear layer\n",
        "\n",
        "Use the following code snippet as your starting point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8HT569Wry2w"
      },
      "outputs": [],
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout_p):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Implement `self.linear1`\n",
        "        # TODO: Implement `self.relu`\n",
        "        # TODO: Implement `self.dropout`\n",
        "        # TODO: Implement `self.linear2`\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x) # pass through LayerNorm\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Implement the rest of the forward pass\n",
        "# ---------------------------------------------------------------------------- #\n",
        "        return out # return the output of the second linear layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFs5G9m5ry2w"
      },
      "source": [
        "After you finish creating the class, run the cell and continue to the next section.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5FBY600ry2w"
      },
      "source": [
        "### Sec 4.2: **Multi-head Attention Layer**\n",
        "\n",
        "<p align = \"justify\">The Multi-Head Attention (MHA) is the source of novelty for the transformer architecture. It's designed to simultaneously attend to information at different positions from different subspaces. The MHA mechanism splits the input into multiple \"heads\", then applies the attention mechanism independently to each head before concatenating the results.</p>\n",
        "\n",
        "<p align = \"justify\">The attention mechanism is a way to weight the input values based on their relevance to a query. This allows the model to focus on the most important parts of the input for each specific task. The MHA allows the model to have multiple such focus points.</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/MHA.png\" alt=\"inputs\" width=\"900\"/>\n",
        "<br></br>\n",
        "\n",
        "##### **Exercise**\n",
        "\n",
        "Now let's implement the MHA using PyTorch. You will need to create a class called MultiHeadAttention, which is a subclass of nn.Module.\n",
        "\n",
        "**You need to provide the class with code to do two things:**\n",
        "\n",
        "1. First, to initialize the various PyTorch layers/functions to the requested sizes within the `__init__()` method of your class.\n",
        "2. Then, create the attention() method, which will compute the scaled dot product attention.\n",
        "3. Finally, transform the incoming data using the method/layers/functions within the `forward()` method of your class.\n",
        "\n",
        "\n",
        "**The MHA class should have the following parameters:**\n",
        "- `self.norm` ([`nn.LayerNorm`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#layernorm)): normalizes the data with shape `input_dim`\n",
        "- `self.dropout` ([`nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout)): has a `dropout_p` probability of an element to be zeroed\n",
        "- `self.linear_q` ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)): transforms the input dimensions from `input_dim` to `self.all_head_dims`, does not use a bias (set optional `bias` parameter to `False`)\n",
        "- `self.linear_k` ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)): transforms the input dimensions from `input_dim` to `self.all_head_dims`, does not use a bias (set optional `bias` parameter to `False`)\n",
        "- `self.linear_v` ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)): transforms the input dimensions from `input_dim` to `self.all_head_dims`, does not use a bias (set optional `bias` parameter to `False`)\n",
        "- `self.linear_out` ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)): transforms the input dimensions from `self.all_head_dims` to `input_dim`\n",
        "\n",
        "<br>\n",
        "\n",
        "To start, define the `attention()` function for this network to compute scaled dot product attention.\n",
        "<br>\n",
        "\n",
        "**This method should:**\n",
        "\n",
        "1. Transpose `key` tensors in the last two dimensions using [`torch.transpose()`](https://pytorch.org/docs/stable/generated/torch.transpose.html) (you can use a negative index to indicate number of points from the end, so -1 would mean the last dimension)\n",
        "2. Compute attention scores by performing matrix multiplication (via: [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html)) between the `query` and the transposed `key` tensors.\n",
        "3. Normalize the attention scores by dividing them by the square root of the number of dimensions in each head.\n",
        "4. Add the `attn_mask` to the attention scores tensor.\n",
        "5. Apply the [`F.softmax()`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) function along the last dimension (-1) to obtain the attention weights.\n",
        "6. Apply dropout to the attention weights tensor.\n",
        "7. Compute the final output by performing matrix multiplication between the attention weights and the value tensor.\n",
        "8. Return the result.\n",
        "\n",
        "Next, within the `forward()` method, you are to finish defining the forward function.\n",
        "<br>\n",
        "\n",
        "**You need to:**\n",
        "\n",
        "1. Pass `x` through the 3 linear layers to get the queries, keys, and values.\n",
        "2. Pass the queries, keys, and values (& attention mask) through the attention function\n",
        "5. Transpose the result in first two dimensions\n",
        "6. Prepare data for final linear layer\n",
        "7. Pass the output through the final linear layer\n",
        "8. Reshape the output to match the input dimensions\n",
        "\n",
        "Use the following code snippet as your starting point:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFI4mH-nry2w"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, head_dims, dropout_p):\n",
        "        super().__init__()\n",
        "        self.head_dims = head_dims\n",
        "        self.num_heads = num_heads\n",
        "        self.all_head_dims = head_dims * num_heads\n",
        "        # the scaling for the scaled dot product attention\n",
        "        self.dpa_demon = math.sqrt(self.head_dims)\n",
        "\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Implement `self.norm`\n",
        "        # TODO: Implement `self.dropout`\n",
        "        # TODO: Implement `self.linear_q` (NOTE: make sure not to use a bias!)\n",
        "        # TODO: Implement `self.linear_k` (NOTE: make sure not to use a bias!)\n",
        "        # TODO: Implement `self.linear_v` (NOTE: make sure not to use a bias!)\n",
        "        # TODO: Implement `self.linear_out`\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "    def attention(self, query, key, value, attn_mask):\n",
        "        # shape of query, key, & value:\n",
        "        #  [(batch_size * n_heads) x time x head_dim]\n",
        "\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: First, transpose the `key` tensor using `torch.transpose()` to\n",
        "        #       get the key into shape:\n",
        "        #         [(batch_size * n_heads) x head_dim x time]\n",
        "        # TODO: Get the attention scores, by performing matrix multiplication\n",
        "        #       (`torch.matmul()`), between `query` and `key`\n",
        "        # TODO: Divide the attention acores by `denom`\n",
        "        # TODO: Add the `attn_mask` to the attention scores\n",
        "        # TODO: Apply `F.softmax()` to the attn_scores on the last dimension to\n",
        "        #       get the attention weights\n",
        "        # TODO: Apply dropout attention weights\n",
        "        # TODO: Get the result, by performing matrix multiplication\n",
        "        #       (`torch.matmul()`), attention weights and `value`\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        # input shape: [B x T x N]\n",
        "        batch_size, seq_len, n_electrodes = x.size()\n",
        "\n",
        "        # shape: [B x T x N] -> [T x B x N]\n",
        "        x = x.transpose(0,1)\n",
        "\n",
        "        # pass the input through the LayerNorm\n",
        "        x = self.norm(x)\n",
        "\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Pass the input through the linear layers to get the queries,\n",
        "        #       keys, and values to transform their shape from: [T x B x N],\n",
        "        #       into: [T x B x (n_heads * head_dims)]\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "        # reshape the queries, keys, and values:\n",
        "        # [T x B x (n_heads * head_dims)] -> [(B * n_heads) x T x head_dim]\n",
        "        transform = lambda x: x.contiguous().view(\n",
        "            seq_len, batch_size*self.num_heads, self.head_dims).transpose(0,1)\n",
        "        query, key, value = map(transform, [query, key, value])\n",
        "\n",
        "        # each sample in the batch needs an attention mask, repeat it\n",
        "        attn_mask = attn_mask.repeat((batch_size*self.num_heads, 1, 1))\n",
        "\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Pass the `query`, `key`, `value`, and `attn_mask` through the\n",
        "        #       attention function you just created and store the result in a\n",
        "        #       variable named `x`\n",
        "        # TODO: Transpose via: `torch.transpose()` to go from shape:\n",
        "        #       [(B * n_heads) x T x head_dims] -> [T x (B * n_heads) x head_dims]\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "        # prepare data for final linear layer, shape: [(T * B) x (n_heads * head_dims)]\n",
        "        x = x.contiguous().view(seq_len * batch_size, self.all_head_dims)\n",
        "\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Pass the output through the final linear layer, to go from\n",
        "        #       shape: [(T * B) x (n_heads * head_dims)] -> [(T * B) x N]\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "        # reshape: [(T * B) x N] -> [B x T x N]\n",
        "        x = x.view(seq_len, batch_size, n_electrodes).transpose(0,1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HmFS4N5Zih_"
      },
      "source": [
        "After you finish creating the class, run the cell and continue to the next section.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCtbrar4ry2x"
      },
      "source": [
        "### Sec 4.3: **Encoder Layer**\n",
        "\n",
        "The Encoder Layer is a building block of the Transformer Encoder. Each encoder layer includes two main parts: a multi-head attention (MHA) mechanism, and a feed forward network (FFN). The input goes through the MHA first, and then the output of MHA is passed to the FFN. There are residual connections and dropout is also applied after the MHA and the FFN.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/Encoder.png\" alt=\"inputs\" width=\"900\"/>\n",
        "<br></br>\n",
        "\n",
        "##### **Exercise**\n",
        "\n",
        "Now let's implement the EncoderLayer using PyTorch. You will need to create a class called EncoderLayer, which is a subclass of nn.Module.\n",
        "\n",
        "You need to provide the class with code to do two things:\n",
        "\n",
        "1. First to initialize the various PyTorch layers/functions to the requested sizes within the __init__ method of your class.\n",
        "2. Then, to transform the incoming data using those layers/functions within the forward method of your class.\n",
        "\n",
        "\n",
        "The EncoderLayer should consist of:\n",
        "\n",
        "- `self.MHA` ([`MultiHeadAttention`](#scrollTo=v5FBY600ry2w&line=3&uniqifier=1)): takes in the requested items from the `config_dict`\n",
        "- `self.FFN` ([`FeedForwardLayer`](#scrollTo=QlHYDUI7ry2w&line=12&uniqifier=1)): takes in the requested items from the `config_dict`\n",
        "- `self.post_mha_dropout` ([`nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout)): has a `config_dict['mha_dropout_p']` probability of an element to be zeroed\n",
        "- `self.post_ffn_dropout` ([`nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout)): has a `config_dict['ffn_dropout_p']` probability of an element to be zeroed\n",
        "\n",
        "<br>\n",
        "\n",
        "Next, within the `forward()` method, you are to define the forward pass for this network.\n",
        "<br>\n",
        "**This method should:**\n",
        "\n",
        "1. Store the input `x` in variable named `residual`\n",
        "2. Pass the output of (1) through the MHA\n",
        "3. Pass the output of (2) through the dropout\n",
        "4. Add the `residual` to the output of (3)\n",
        "5. Store the output of (4) in variable named `residual`\n",
        "6. Pass the output of (5) through the FFN\n",
        "7. Pass the output of (6) through the dropout\n",
        "8. Add the `residual` to the output of (7)\n",
        "9. Return the output of (8)\n",
        "\n",
        "Use the following code snippet as your starting point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGemaUjOry2x"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config_dict):\n",
        "        super().__init__()\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Implement `self.MHA` with the following parameters:\n",
        "        #         input_dim = config_dict['factor_dim'],\n",
        "        #         num_heads = config_dict['n_heads'],\n",
        "        #         head_dims = config_dict['head_dims'],\n",
        "        #         dropout_p = config_dict['mha_dropout_p']\n",
        "        # TODO: Implement `self.FFN` with the following parameters:\n",
        "        #         input_dim = config_dict['factor_dim'],\n",
        "        #         hidden_dim = config_dict['ffn_dim'],\n",
        "        #         dropout_p = config_dict['ffn_dropout_p']\n",
        "        # TODO: Implement `self.post_mha_dropout`\n",
        "        # TODO: Implement `self.post_ffn_dropout`\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Implement the forward pass\n",
        "# ---------------------------------------------------------------------------- #\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi5rrpYfZlw4"
      },
      "source": [
        "After you finish creating the class, run the cell and continue to the next section.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfImA1Smry2x"
      },
      "source": [
        "\n",
        "### Sec 4.4: **The Full Model**\n",
        "\n",
        "<p align = \"justify\">The full <b>Neural Data Transformer Model</b> extends the single layer architecture discussed in the previous section to a full transformer model with multiple layers. This model's structure includes a positional embedding layer, a multi-layer encoder stack, and an output layer that maps the transformed factors back to the electrode space.</p>\n",
        "\n",
        "<p align = \"justify\">The positional embedding layer is necessary because the transformers, by design, do not understand the order of tokens (electrodes in our case). This positional embedding layer adds information about the token's position in the sequence to the token embeddings themselves.</p>\n",
        "\n",
        "<p align = \"justify\">The transformer encoder stack is a sequence of multiple identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network.In the output layer, we project back the factors to the original space of the electrodes. When training, this model also calculates the Poisson log-likelihood loss based on the original and the predicted spikes. In addition, it implements dropout regularization to prevent overfitting.</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/full_model.png\" alt=\"inputs\" width=\"1000\"/>\n",
        "<br></br>\n",
        "\n",
        "##### **Exercise**\n",
        "Now let's implement the Full Neural Data Transformer Model using PyTorch. You will need to create a class called NeuralDataTransformer, which is a subclass of nn.Module.\n",
        "\n",
        "**You need to provide the class with code to do two things:**\n",
        "1. First to initialize the various PyTorch layers/functions to the requested sizes within the `__init__` method of your class.\n",
        "2. Then, to transform the incoming data using those layers/functions within the `forward()` method of your class.\n",
        "\n",
        "<b>You will need to implement the following parameters of the NeuralDataTransformer:</b>\n",
        "- `self.readin` ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)): An input layer that reduces dimensionality from `n_electrodes` to `factor_dim`.\n",
        "- `self.readout` ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)): An input layer that increase dimensionality from `factor_dim` to `n_electrodes`.\n",
        "- `encoder_layer_list` (list of `EncoderLayer`s): fill the list with the same number of `EncoderLayer` objects as there are layers, we will pass the input through each item in this list to pass the input through the different \"layers\".\n",
        "<br>\n",
        "\n",
        "Next, within the `forward()` method, you are to define the outlined steps for this layer.\n",
        "<br>\n",
        "**You should:**\n",
        "\n",
        "1. Pass the input `x` through the readin\n",
        "2. Pass the output of (1) through the post readin dropout layer\n",
        "3. Add the positional embedding (`self.pos_embedding`) to the output of (2)\n",
        "4. Pass the output of (3) through the post embedding dropout layer\n",
        "5. Pass the output of (4) through the encoder layer with the attention mask\n",
        "6. Pass the output of (5) through the normalization layer\n",
        "7. Pass the output of (6) through the dropout layer\n",
        "8. Pass the output of (7) through the readout layer\n",
        "\n",
        "Use the following code snippet as your starting point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM5UGVVtry2x"
      },
      "outputs": [],
      "source": [
        "class NeuralDataTransformer(nn.Module):\n",
        "    def __init__(self, config_dict):\n",
        "        super().__init__()\n",
        "        # layers of the model\n",
        "        self.config_dict = config_dict\n",
        "        self.context_forward = config_dict['context_forward']\n",
        "        self.context_backward = config_dict['context_backward']\n",
        "        encoder_layer_list = []\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Implement `self.readin`\n",
        "        # TODO: Implement `self.readout`\n",
        "        # TODO: create a list of `EncoderLayer()` objects by adding a new object\n",
        "        #       to the list for each layer (`config_dict['n_layers']` in total)\n",
        "# ---------------------------------------------------------------------------- #\n",
        "        self.encoder = nn.ModuleList(encoder_layer_list)\n",
        "\n",
        "        # transform data\n",
        "        self.norm = nn.LayerNorm(config_dict['factor_dim'])\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([config_dict['factor_dim']]))\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(config_dict['seq_len'], config_dict['factor_dim']))\n",
        "\n",
        "        # for training\n",
        "        self.classifier = nn.PoissonNLLLoss(reduction='none')\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Implement `self.post_embedding_dropout`\n",
        "        # TODO: Implement `self.post_readin_dropout`\n",
        "        # TODO: Implement `self.post_encoder_dropout`\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "        # init\n",
        "        self.readout.bias.data.zero_()\n",
        "        self.readout.weight.data.uniform_(-0.01, 0.01)\n",
        "        self.readin.weight.data.uniform_(-0.01, 0.01)\n",
        "        self.pos_embedding.requires_grad = True\n",
        "        nn.init.xavier_uniform_(self.pos_embedding)\n",
        "\n",
        "        # cache for training\n",
        "        self.attn_mask = None\n",
        "        self.zero_prob_mask = None\n",
        "        self.loss_prob_mask = None\n",
        "        self.random_prob_mask = None\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Pass `x` through the readin\n",
        "        # TODO: Pass `x` through the post readin dropout layer\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "        # scale the inputs so that the positional embedding doesn't \"overpower\" data\n",
        "        x *= self.scale.to(x.device)\n",
        "\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # TODO: Add the positional embedding to `x`\n",
        "        # TODO: Pass `x` through the post readin dropout layer\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "        # get the attention mask\n",
        "        attn_mask = self.get_attn_mask(x)\n",
        "\n",
        "# ----------------------------- Student Section ------------------------------ #\n",
        "        # pass through each of the encoder layers\n",
        "        for encoder_layer in self.encoder:\n",
        "            attn_mask = attn_mask.to(x.device)\n",
        "            # TODO: Pass `x` through the encoder layer\n",
        "        # TODO: Pass `x` through the normalization layer\n",
        "        # TODO: Pass `x` through the post encoder dropout layer\n",
        "        # TODO: Pass `x` through the readout\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "        # if we are not training then there is no loss, return the rates\n",
        "        if labels == None:\n",
        "            return torch.exp(x)\n",
        "\n",
        "        # if we are training, then get the loss\n",
        "        loss = self.classifier(x, labels)\n",
        "\n",
        "        # only backprop loss from the masked samples\n",
        "        masked_loss = loss[labels != -100]\n",
        "\n",
        "        # get a single value\n",
        "        masked_loss = masked_loss.mean()\n",
        "\n",
        "        # return both loss and rates\n",
        "        return masked_loss, torch.exp(x)\n",
        "\n",
        "    def get_attn_mask(self, src):\n",
        "        ''' Ignore this. '''\n",
        "        # use cached version if already created.\n",
        "        if self.attn_mask != None:\n",
        "            return self.attn_mask\n",
        "\n",
        "        ones = torch.ones(src.size(1), src.size(1), device=src.device)\n",
        "        forw_mask = (torch.triu(ones, diagonal=-self.context_forward) == 1).transpose(0, 1)\n",
        "        back_mask = (torch.triu(ones, diagonal=-self.context_backward) == 1)\n",
        "        mask = (forw_mask & back_mask).float()\n",
        "        mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        mask = mask.unsqueeze(0)\n",
        "        self.attn_mask = mask # cache the mask\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def preprocess_batch(self, batch, heldout_spikes, forward_spikes):\n",
        "        ''' Ignore this. '''\n",
        "        batch = batch.clone()\n",
        "        labels = batch.clone()\n",
        "        loss_ratio = self.config_dict['loss_ratio']\n",
        "\n",
        "        # Which indicies shold the loss be computed with\n",
        "        if self.loss_prob_mask is None or self.loss_prob_mask.size() != labels.size():\n",
        "            timestep_shape = labels[:, :, 0].shape # N x T\n",
        "            self.loss_prob_mask = torch.full(timestep_shape, loss_ratio, device=batch.device, dtype=torch.float32)\n",
        "        loss_mask = torch.bernoulli(self.loss_prob_mask)\n",
        "        loss_mask = loss_mask.bool().unsqueeze(2).expand_as(labels)\n",
        "        labels[~loss_mask] = -100\n",
        "\n",
        "        # Zero mask\n",
        "        if self.zero_prob_mask is None or self.zero_prob_mask.size() != labels.size():\n",
        "            zero_mask_ratio = self.config_dict['mask_ratio']\n",
        "            self.zero_prob_mask = torch.full(labels.shape, zero_mask_ratio, device=batch.device, dtype=torch.float32)\n",
        "        indices_zero_masked = torch.bernoulli(self.zero_prob_mask).bool() & loss_mask\n",
        "        batch[indices_zero_masked] = 0\n",
        "\n",
        "        # Randomize\n",
        "        if self.random_prob_mask is None or self.random_prob_mask.size() != labels.size():\n",
        "            randomize_ratio = self.config_dict['random_ratio']\n",
        "            self.random_prob_mask = torch.full(labels.shape, randomize_ratio, device=batch.device, dtype=torch.float32)\n",
        "        indices_randomized = torch.bernoulli(self.random_prob_mask).bool() & loss_mask & ~indices_zero_masked\n",
        "        random_spikes = torch.randint(batch.max().long(), labels.shape, dtype=torch.long, device=batch.device)\n",
        "        batch[indices_randomized] = random_spikes.float()[indices_randomized]\n",
        "\n",
        "        # Add zeros for heldout and forward\n",
        "        batch = torch.cat([batch, torch.zeros_like(heldout_spikes)], -1)\n",
        "        labels = torch.cat([labels, heldout_spikes], -1)\n",
        "        batch = torch.cat([batch, torch.zeros_like(forward_spikes)], 1)\n",
        "        labels = torch.cat([labels, forward_spikes], 1)\n",
        "\n",
        "        return batch, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKpKY-2kZott"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo27mQxvry2x"
      },
      "source": [
        "## Section 5: Model Training & Evaluation\n",
        "\n",
        "##### **Introductory ML Terms:**\n",
        "\n",
        "* <p align = \"justify\"><a href=\"https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss\"><b><i>Loss</b></i></a>: a numerical measure of how far the model's predictions are from the actual values, used during training to guide the improvement of the model. This value is determined by the <i>loss function</i>. For training the NDT model we will be using the Negative log likelihood loss (with a Poisson distribution of target) or <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html\"><code>nn.PoissonNLLLoss()</code></a> between the inferred firing rates of the neurons and the true spiking activity of the neurons.</p>\n",
        "\n",
        "* <p align = \"justify\"><a href=\"https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\"><b><i>Batch</b></i></a>: a subset of the dataset that is used for training the model in a single step. The model's parameters are updated after each batch. Batch size is a crucial hyperparameter that affects the model's performance and speed of training.</p>\n",
        "\n",
        "* <p align = \"justify\"><a href=\"https://www.geeksforgeeks.org/epoch-in-machine-learning/\"><b><i>Epoch</b></i></a>: an epoch is one complete pass through the all of the batches (the entire training dataset) during the training process.</p>\n",
        "\n",
        "* <p align = \"justify\"><a href=\"https://www.kdnuggets.com/2021/03/machine-learning-learning-rate.html\"><b><i>Learning Rate</b></i></a>: the learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.</p>\n",
        "* <p align = \"justify\"><a href=\"https://www.deeplearningbook.org/contents/optimization.html\"><b><i>Optimizer</b></i></a>: the optimization algorithm used to update the weights of the model based on the gradients of the loss function. Examples of optimizers are Stochastic Gradient Descent (SGD), Adam, and RMSprop.</p>\n",
        "\n",
        "#### **MC Maze Dataset**:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/maze.png\" alt=\"inputs\" width=\"1100\"/>\n",
        "\n",
        "> *Example of the 4 stages in the delayed center out reaching task.*\n",
        "<br>\n",
        "\n",
        "- <p align = \"justify\">The <code>MC_Maze</code> dataset includes neural activity recorded from the primary motor and dorsal premotor cortices of a macaque performing a delayed center-out reach task in various maze configurations. This task results in a variety of straight and curved trajectories, and the predictability of the neural activity during these highly-stereotyped movements makes the dataset useful for evaluating a model's ability to represent autonomous dynamics.</p>\n",
        "\n",
        "- <p align = \"justify\">The dataset that we will be using today, <code>MC_Maze_Small</code>, is a variant of the <code>MC_Maze</code> dataset with a limited number of trials, designed to assess how modeling methods scale to datasets with restricted amounts of data.</p>\n",
        "\n",
        "- <p align = \"justify\">This data was collected by Krishna Shenoy, Mark Churchland, and Matt Kaufman from Stanford University. If you are interested in learning more about the dataset,  check out <a href=\"https://github.com/neurallatents/neurallatents.github.io/blob/master/notebooks/mc_maze.ipynb\">this</a> wonderful notebook by Felix Pei that breaks down the specifics of the dataset. </p>\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align = \"justify\">The main goal of the benchmark is to <b>predict the activity (firing rates) of a set of (\"held-out\") neurons, given the activity of another set of (\"held-in\") neurons</b>. Secondary goals include the prediction of future timesteps (how would the activity of both the held-in and held-out neurons continue to evolve), and how well the kinematics can be linearly decoded from the predicted firing rates.</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQS05gC77tbm"
      },
      "source": [
        "### Sec 5.1: **Process the Data**\n",
        "\n",
        "Let's now import the data we downloaded during the setup. The format used is [NWB](https://nwb-overview.readthedocs.io/en/latest/intro_to_nwb/1_intro_to_nwb.html). We will resample the data to 5ms bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wvQpwWQ4Svh"
      },
      "outputs": [],
      "source": [
        "# curr_path = os.getcwd()\n",
        "# os.listdir(curr_path)\n",
        "\n",
        "## Get the current path and pull the dataset into the Neurodata Without Borders format\n",
        "fpath = '/content/000140/sub-Jenkins'\n",
        "dataset = NWBDataset(fpath=fpath)\n",
        "dataset.resample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IktIdnVG4zu0"
      },
      "source": [
        "Now we will get the dataloaders, `utils.get_dataloaders()` uses functions created for the NLB to extract and process the data. The function can be found [here](https://github.com/domenick-m/Caltech_DSAINSS_23/blob/main/utils.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B02_cV0SqG2E"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "dataloaders = utils.get_dataloaders(dataset, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJRXRkT-p9KF"
      },
      "source": [
        "### Sec 5.2: **Model Training**\n",
        "\n",
        "We will now begin the training process. We first need to initialize the model, we do that using a configuration file that defines all of our hyperparameters. It is common to use the dictionary structure for this as it allows you to store mulitple type of variables using strings to access them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TF1iLbTp67e"
      },
      "outputs": [],
      "source": [
        "config_dict = dict(\n",
        "    n_electrodes = 142,\n",
        "    seq_len = 180,\n",
        "\n",
        "    n_layers = 4,\n",
        "    n_heads = 2,\n",
        "    factor_dim = 64,\n",
        "    head_dims = 32,\n",
        "    ffn_dim = 128,\n",
        "    context_forward = 15,\n",
        "    context_backward = 35,\n",
        "\n",
        "    post_readin_dropout_p = 0.2,\n",
        "    post_embedding_dropout_p = 0.7,\n",
        "    post_encoder_dropout_p = 0.2,\n",
        "    mha_dropout_p = 0.5,\n",
        "    ffn_dropout_p = 0.3,\n",
        "\n",
        "    loss_ratio=0.4,\n",
        "    mask_ratio=0.75,\n",
        "    random_ratio=1.0,\n",
        "    weight_decay = 0.000005,\n",
        "    lr_init = 0.005,\n",
        "    n_epochs=2500,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n4avg1ug6g-"
      },
      "source": [
        "We can now initialize the model, and create the optimizer for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_pecQ5CqNJN"
      },
      "outputs": [],
      "source": [
        "model = NeuralDataTransformer(config_dict)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr = config_dict['lr_init'],\n",
        "                              weight_decay = config_dict['weight_decay'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0RYE0o5hD1C"
      },
      "source": [
        "We set the seeds so that we can get more repeatable results, it is not 100% deterministic as it slows down training significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhAwa5vvRMGL"
      },
      "outputs": [],
      "source": [
        "utils.set_seeds(12345)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qokjFVSRhW92"
      },
      "source": [
        "Now the full training loop. We have it set to 2500 epochs and this should take around 2 minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqZtEcTZ9avm"
      },
      "outputs": [],
      "source": [
        "# initialize lists to keep track of the losses\n",
        "losses, eval_losses = [], []\n",
        "\n",
        "# move the model to the GPU where the computations will be performed.\n",
        "model = model.to(device)\n",
        "\n",
        "# create progress bars for visualization during training\n",
        "prog_bar, loss_bar = utils.get_prog_bars(config_dict['n_epochs'])\n",
        "\n",
        "# the main training loop. we run for a certain number of epochs.\n",
        "for epoch in prog_bar:\n",
        "    # set the model to training mode. this affects certain operations which behave differently during train vs evaluation.\n",
        "    model.train()\n",
        "\n",
        "    # load data from the training set.\n",
        "    for heldin, recon, vel, conds in dataloaders['train']:\n",
        "        # preprocess the batch data using a model's specific function.\n",
        "        batch, labels = model.preprocess_batch(\n",
        "            heldin,\n",
        "            recon[:, :140, 107:],\n",
        "            recon[:, 140:, :]\n",
        "        )\n",
        "\n",
        "        # forward pass through the model. the model returns a loss and the reconstructed output.\n",
        "        loss, reconstructed = model(batch, labels)\n",
        "\n",
        "        # compute gradients based on the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # update model parameters using computed gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # reset gradients to zero for the next batch\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # detach loss from computation graph and move it to cpu for storage\n",
        "        batch_loss = loss.detach().cpu().numpy()\n",
        "\n",
        "        # save the loss for this batch\n",
        "        losses.append([epoch, batch_loss])\n",
        "\n",
        "    # switch model to evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    # every 2 epochs, evaluate model on validation set.\n",
        "    if np.mod(epoch, 2)==0:\n",
        "        # list to store losses for each batch in the current epoch\n",
        "        epoch_eval_loss = []\n",
        "\n",
        "        # load data from the validation set\n",
        "        for heldin, recon, vel, conds in dataloaders['val']:\n",
        "            # ensure that no gradients are calculated since we're not training\n",
        "            with torch.no_grad():\n",
        "                spikes = torch.zeros_like(recon)\n",
        "                spikes[:, :140, :107] = heldin\n",
        "\n",
        "                # forward pass\n",
        "                loss, reconstructed = model(spikes, recon)\n",
        "\n",
        "                # append the current batch's loss to the list\n",
        "                epoch_eval_loss.append(loss.cpu().numpy())\n",
        "\n",
        "        # compute mean validation loss for the epoch\n",
        "        mean_loss = np.mean(epoch_eval_loss)\n",
        "\n",
        "        # save the mean validation loss for this epoch\n",
        "        eval_losses.append([epoch, mean_loss])\n",
        "\n",
        "        # update the description of the loss bar with latest training and validation loss\n",
        "        loss_bar.set_description_str(f\"[train loss: {batch_loss:.4f}]   [val loss: {mean_loss:.4f}]\")\n",
        "\n",
        "# close the progress bars after training\n",
        "prog_bar.close()\n",
        "loss_bar.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVFrLZ0X1GTZ"
      },
      "source": [
        "While your model is training, let's break down some of the model training code more...\n",
        "\n",
        "In Pytorch, the typical model training routine includes several steps:\n",
        "\n",
        "1. **Sending the model to a device**: `model = model.to(device)`. This is usually done to transfer the model to the GPU, if available, for faster computations.\n",
        "\n",
        "2. **Setting the model to training mode**: `model.train()`. This ensures that certain layers of the model, like dropout layers, behave differently during training and evaluation.\n",
        "\n",
        "3. **Loading batches of data**: `for heldin, recon, vel, conds in dataloaders['train']:`. This loop iterates through batches of data in the training set.\n",
        "\n",
        "4. **Preprocessing the batch**: `batch, labels = model.preprocess_batch(...)`. Depending on the model and the task, preprocessing might involve various steps like reshaping the data, converting data types, or normalizing values.\n",
        "\n",
        "5. **Forward pass**: `loss, reconstructed = model(batch, labels)`. This step runs the model on the input data and computes the loss.\n",
        "\n",
        "6. **Backward pass**: `loss.backward()`. This computes the gradients of the loss with respect to the model parameters.\n",
        "\n",
        "7. **Parameter update**: `optimizer.step()`. This updates the model parameters using the computed gradients.\n",
        "\n",
        "8. **Zeroing the gradients**: `optimizer.zero_grad(set_to_none=True)`. This clears the gradients for the next iteration.\n",
        "\n",
        "9. **Validation**: The block of code under `model.eval()` is where the validation occurs. This is similar to the training loop but does not include the backward pass or parameter update steps, because the model is not updated during validation.\n",
        "\n",
        "10. **Logging**: Throughout this process, various metrics like the training and validation losses are logged for analysis.\n",
        "\n",
        "Now, once the model finishes training let's take a look at the loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7TEo-_JtQcp"
      },
      "outputs": [],
      "source": [
        "# split the lists into the epoch number and the loss\n",
        "losses_0, losses_1 = np.array(losses).T\n",
        "eval_losses_0, eval_losses_1 = np.array(eval_losses).T\n",
        "\n",
        "# The rest of the code sets up the plot.\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.scatter(losses_0, losses_1, label='train')\n",
        "plt.scatter(eval_losses_0, eval_losses_1, label='eval')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.ylim(0.07, 0.1) # we manually set the limits, if you cannot see your loss remove this line\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXbMYNrrq5tm"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbTRWrOPrBxJ"
      },
      "source": [
        "### Sec 5.3: **Model Evaluation**\n",
        "\n",
        "Now that we have our trained model, we should look at how the models predictions of the firing rates compare to simply smoothing the spikes. Before we do that however, let's briefly take a closer look at the dataset (`mc_maze_small`) that we have been using so far. To do that, we will first \"trialize\" the continous data by taking a range (in our case 250 ms before to 450 ms after) around a key event in each trial, we will be using movement onset (when the monkeys hand starts moving) as that event."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzrD9B-vbcLk"
      },
      "outputs": [],
      "source": [
        "# split the data up into trials aligned to the movement onset\n",
        "trial_data = dataset.make_trial_data(align_field = \"move_onset_time\", align_range = (-250, 450))\n",
        "trial_data = utils.add_conds_to_trial_data(trial_data_in= trial_data, dataset_in= dataset)\n",
        "\n",
        "# get the trialized conditions, held-in spikes, hand velocities, and hand positions\n",
        "grouped = list(trial_data.groupby('trial_id', sort=False))\n",
        "conds = np.stack([trial[\"trial_cond\"].to_numpy() for _, trial in grouped])[:,0]\n",
        "heldin = torch.tensor(np.stack([trial['spikes'].to_numpy() for _, trial in grouped]))\n",
        "vel = torch.tensor(np.stack([trial['hand_vel'].to_numpy() for _, trial in grouped]))\n",
        "pos = torch.tensor(np.stack([trial['hand_pos'].to_numpy() for _, trial in grouped]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rZW0MOzbfKV"
      },
      "source": [
        "Let's take a look at all the trials in some exanple conditions to see what the reaches actually look like for the first 325 ms after movement onset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLbIvcwPbhUe"
      },
      "outputs": [],
      "source": [
        "cond_list = [1, 4, 7, 13] # example conditions to plot, feel free to change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL0e4k-_biDo"
      },
      "outputs": [],
      "source": [
        "utils.plot_hand_vel(cond_list, pos, conds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Cm2lsubjrK"
      },
      "source": [
        "Great! Now let's take a look at the binned spikes and the monkeys hand velocity across time for one of the trials. Take a minute to see if you can find any obvious relationships between the neural activity and kinematics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxMHkG_7bpDg"
      },
      "outputs": [],
      "source": [
        "trial_id = 5\n",
        "utils.plot_ex_spikes_vel(heldin, vel[trial_id,:140], trial_id, conds[trial_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUNPP86bbsn4"
      },
      "source": [
        "Looking at <i>just</i> the spikes, it is very difficult to find any meaningful patterns.\n",
        "\n",
        "To get a better picture of how a neurons activity relates to the hand kinematics, we first need to get an estimate of the instantaneous firing rates for all the neurons. We can do that by smoothing with a Gaussian filter. Smoothing however cannot fix the fact that spikes are inherintley a noisy estimate of the underlying firing rates. If we have enough repeated trials of the same conditions, we can average out some of this noise by taking the mean of the smoothed spiking activity across all those trials. This is referred to as a *Peristimulus time histogram* (PSTH).\n",
        "\n",
        "First we can set the amount of smoothing we want as well as the example neurons:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX-Mww0IbuhL"
      },
      "outputs": [],
      "source": [
        "kernel_size =  10 # number of 5ms bins in filter to smooth with\n",
        "neuron_list = [1, 3, 19, 23, 25, 27, 31, 53, 59, 68, 69, 76, 84, 87, 101]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7pcHnR9bu84"
      },
      "source": [
        "Let's now create PSTHs of our smoothed binned spikes for 15 example neurons (using the example conditions from earlier):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGyl57MSbxGn"
      },
      "outputs": [],
      "source": [
        "utils.plot_cond_avg_fr(trial_data, cond_list, neuron_list, kernel_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJfCRDUTbzji"
      },
      "source": [
        "With the PSTHs we can see some clear differences in the average activity (light colored area is STD) across the example conditions. Let's verify that our model has learned a similar structure in the PSTHs. We first need to run inference on the complete training set with our trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJt9XITHb0AM"
      },
      "outputs": [],
      "source": [
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "model = model.cpu()\n",
        "\n",
        "# run inference with the model on the spikes (after adding 0s for heldout data)\n",
        "bs, time, n_heldin = heldin.shape\n",
        "spikes = torch.cat((heldin, torch.zeros((bs,time,35))), -1)\n",
        "spikes = torch.cat((spikes, torch.zeros((bs,40,142))), 1)\n",
        "rates = model(spikes).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEAMjZK6b0a8"
      },
      "source": [
        "Now we can plot the PSTHs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CgrKPab0u_"
      },
      "outputs": [],
      "source": [
        "utils.plot_cond_avg_pred_fr(rates, conds, cond_list, neuron_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jBo3w4-b1Fn"
      },
      "source": [
        "Wow! Our model has seemingly found similar trends as the PSTHs, but they are a lot more separated by condition. The advantage that our model has over the PSTHs though is that we dont need to condition average our estimated rates to get a clean signal. This is especially useful in applications such as BCIs when we need to instanly predict an intended velocity.\n",
        "\n",
        "Let's see what single single trial smoothed spikes like and take a second see if the relationships we indentified above would still be visible if we were trying to predict which condition the monkey is in just using the single trial firing rates (smoothed spikes):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5LPeRX0b1hO"
      },
      "outputs": [],
      "source": [
        "utils.plot_st_fr(trial_data, cond_list, neuron_list, kernel_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM1tWfKMb12w"
      },
      "source": [
        "While there is some separability amost the traces of the different conditions, most neurons seem to give pretty similar responses across all conditions which is less than ideal for trying to decode in real-time.\n",
        "\n",
        "Let's now take a look at the single trial firing rate estimates produced by our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rEf-L_ccG2T"
      },
      "outputs": [],
      "source": [
        "utils.plot_pred_st_fr(rates, conds, cond_list, neuron_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZfnRKovcHaI"
      },
      "source": [
        "Wow! Even on the single trial level our model gives us very distinct responses to the different conditions. To actually get an estimate of the improvements we can get over smoothed spikes, let's evaluate our model on the validation set (unseen while training) using functions from `nlb_tools`.\n",
        "\n",
        "#### **Validation set evaluation**\n",
        "\n",
        "First run inference with the model on both the training and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MCEt29ecHxM"
      },
      "outputs": [],
      "source": [
        "# Retrieve inputs for submission from dataset\n",
        "(training_input, training_output, eval_input, tlen, num_heldin) = utils.get_submission_inputs(dataset, 'train_val')\n",
        "\n",
        "# Set model to evaluation mode and move to CPU\n",
        "model.eval()\n",
        "model = model.cpu()\n",
        "\n",
        "# Get tensor shape, append zeros to increase last dimension by 35 and make predictions\n",
        "bs, time, heldin = training_input.shape\n",
        "spikes = torch.cat((training_input, torch.zeros((bs,time,35))), -1)\n",
        "training_predictions = model(spikes).cpu().detach().numpy()\n",
        "\n",
        "# Repeat previous steps for evaluation input\n",
        "bs, time, heldin = eval_input.shape\n",
        "spikes = torch.cat((eval_input, torch.zeros((bs,time,35))), -1)\n",
        "eval_predictions = model(spikes).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxdqrzYvcIH1"
      },
      "source": [
        "We will now evaluate how well the model does on the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3Iz0oE-p18R"
      },
      "outputs": [],
      "source": [
        "# Prepare submission data\n",
        "submission = {\n",
        "    'mc_maze_small': {\n",
        "        'train_rates_heldin': training_predictions[:, :tlen, :num_heldin],\n",
        "        'train_rates_heldout': training_predictions[:, :tlen, num_heldin:],\n",
        "        'eval_rates_heldin': eval_predictions[:, :tlen, :num_heldin],\n",
        "        'eval_rates_heldout': eval_predictions[:, :tlen, num_heldin:],\n",
        "        'eval_rates_heldin_forward': eval_predictions[:, tlen:, :num_heldin],\n",
        "        'eval_rates_heldout_forward': eval_predictions[:, tlen:, num_heldin:]\n",
        "    }\n",
        "}\n",
        "target_dict = make_eval_target_tensors(dataset=dataset,\n",
        "                                       dataset_name='mc_maze_small',\n",
        "                                       train_trial_split='train',\n",
        "                                       eval_trial_split='val',\n",
        "                                       include_psth=True,\n",
        "                                       save_file=False)\n",
        "\n",
        "evaluate(target_dict, submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1i90VWBkA_e"
      },
      "source": [
        "<p align = \"justify\">Cool! but what do those numbers even mean? Well to get a sense of how you are doing on the test set, take a look at how you compare to the leaderboard rankings for the <code>mc_maze_small</code> dataset (you should look for the columns that have <code>[100]</code> on them) on <a href=\"https://eval.ai/web/challenges/challenge-page/1256/leaderboard/3184\">EvalAI</a>.</p>\n",
        "\n",
        "Let's briefly go over what each of the metrics mean,\n",
        "\n",
        "- <p align = \"justify\"><b>Co-BPS</b>: Co-BPS assess the performance of predicting the firing rates of a subset of channels based on the activity of other channels, using a metric called bits per spike, which assumes a Poisson likelihood model.</p>\n",
        "\n",
        "- <p align = \"justify\"><b>Vel $R^2$</b>: Velocity $R^2$ involves the use of ridge regression to map rate predictions of all channels to various behavioral metrics such as hand or finger velocity or neural trajectory speed, evaluated using the R^2 metric.</p>\n",
        "\n",
        "- <p align = \"justify\"><b>PSTH R</b>2: PSTH R2 calculates the R^2 metric between test split rate predictions and empirical Peri-Stimulus Time Histograms (PSTHs), which are generated by averaging smoothed spikes from all trials excluding the one being evaluated, with conditions varying based on the dataset.</p>\n",
        "\n",
        "- <p align = \"justify\"><b>FP-BPS</b>: FP-BPS measures a model's ability to predict future dynamics by assessing rate predictions for the next 200 milliseconds following the provided test split trial segments, using the same bits per spike calculation as in Co-BPS.</p>\n",
        "\n",
        "#### **Test set evaluation**\n",
        "\n",
        "Up until now we have just looked at the quality of the model on the validation set, but the true test of the model is how well it can generalize to unseen data. This is dont by comparing models on the [NLB public leaderboard](https://eval.ai/web/challenges/challenge-page/1256/leaderboard/3184).\n",
        "\n",
        "To submit out model to the NLB we need to run the code below. This will do the same thing that we did above, but now we are also running inference on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn4eEsWJN2Kf"
      },
      "outputs": [],
      "source": [
        "# Retrieve inputs for submission from dataset\n",
        "(training_input, training_output, eval_input, tlen, num_heldin) = utils.get_submission_inputs(dataset, 'test')\n",
        "\n",
        "# Set model to evaluation mode and move to CPU\n",
        "model.eval()\n",
        "model = model.cpu()\n",
        "\n",
        "# Get tensor shape, append zeros to increase last dimension by 35 and make predictions\n",
        "bs, time, heldin = training_input.shape\n",
        "spikes = torch.cat((training_input, torch.zeros((bs,time,35))), -1)\n",
        "training_predictions = model(spikes).cpu().detach().numpy()\n",
        "\n",
        "# Repeat previous steps for evaluation input\n",
        "bs, time, heldin = eval_input.shape\n",
        "spikes = torch.cat((eval_input, torch.zeros((bs,time,35))), -1)\n",
        "eval_predictions = model(spikes).cpu().detach().numpy()\n",
        "\n",
        "# Prepare submission data\n",
        "submission = {\n",
        "    'mc_maze_small': {\n",
        "        'train_rates_heldin': training_predictions[:, :tlen, :num_heldin],\n",
        "        'train_rates_heldout': training_predictions[:, :tlen, num_heldin:],\n",
        "        'eval_rates_heldin': eval_predictions[:, :tlen, :num_heldin],\n",
        "        'eval_rates_heldout': eval_predictions[:, :tlen, num_heldin:],\n",
        "        'eval_rates_heldin_forward': eval_predictions[:, tlen:, :num_heldin],\n",
        "        'eval_rates_heldout_forward': eval_predictions[:, tlen:, num_heldin:]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save the submission to disk so that we can submit to EvalAI\n",
        "from nlb_tools.make_tensors import save_to_h5\n",
        "save_to_h5(submission, 'submission.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g1kbzq0OoZ6"
      },
      "source": [
        "If you are interested in submitting your model to the leaderboard (private submissions are allowed!) first create an account on [EvalAI](https://eval.ai/auth/signup). Then download the `submission.h5` file that should be stored in your google colab `contents/` folder. Then, you can go [here](https://eval.ai/web/challenges/challenge-page/1256/submission) to submit the model. Select the \"Test Phase\" and upload the `submission.h5` file. You can go [here](https://eval.ai/web/challenges/challenge-page/1256/my-submission) to look at how your model did by looking at the \"Result file\" after your models evaluation is done.\n",
        "\n",
        "And now here is the test set performance for the model that I created using this notebook:\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/domenick-m/Caltech_DSAINSS_23/main/Images/NLB%20rankings.png\" alt=\"inputs\" width=\"700\"/>\n",
        "\n",
        "We can see that this model does decently when compared to the orginal NDT submission for this dataset on EvalAI. And this was with no hyperparameter tuning / searches and a small number of epochs (relative to the 50,000 max epochs for the [Neural Latents NDT](https://github.com/snel-repo/neural-data-transformers/blob/98dd85a24885ffb76adfeed0c2a89d3ea3ecf9d1/configs/mc_maze_small.yaml#L25)).\n",
        "\n",
        "<br></br>\n",
        "#### **Next Steps:**\n",
        "The next steps are up to you! You could try changing the hyperparameters of the model and retraining it to see if you can get an even higher score on the leaderboard. Another option is to try with your own data, the genral principle of attention and the transformer architecture can be applied out of the box to many neuroscience problems like it was here today.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9xw7EsHIHXK"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NULBeZtrZ3SY"
      },
      "source": [
        "Done!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XXQGYe_bCbA3",
        "UiEzn_c7KhkF",
        "xoLLmiF7l1LQ",
        "GYCEKJqEry2u",
        "nGG8nuAHry2u"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
