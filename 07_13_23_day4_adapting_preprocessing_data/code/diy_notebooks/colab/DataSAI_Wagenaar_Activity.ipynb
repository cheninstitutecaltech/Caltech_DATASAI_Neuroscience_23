{"cells":[{"cell_type":"markdown","metadata":{"id":"5DscXyIA2C0Y"},"source":["# Analyzing spiking responses to visual stimuli\n","\n","In this notebook, you will engage in some exploratory data analysis of one of Frank's recording sessions, the same one we used for our spike sorting exercise this morning.\n","\n","This is a recording from area V2 in tree shrew visual cortex. Not a lot has been published about the neurons in this area. The recording contains responses to several kinds of visual stimuli as well as to electrical stimuli delivered to V1.\n","\n","The point of the exercise is not to reach a scientific conclusion. (After all, this is *N* = 1 recording), but rather to try a few different analytic approaches and see if you can come up with some preliminary ideas about what this neurons in tree shrew V2 are all about. This exercise is intentionally open-ended. We encourage you to try alternative approaches."]},{"cell_type":"markdown","metadata":{"id":"40wev-8k3cHZ"},"source":["## Accessing the data\n","\n","First, we need to access the data. This morning, we already accessed the raw traces for this experiment, now we will also open the KiloSort 2.0 spike data. If you decided you like one of the other sorters better, you can absolutely use its results instead. (In the \"Sorted\" notebook, you can see how to extract the spike trains.)"]},{"cell_type":"markdown","metadata":{},"source":["[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cheninstitutecaltech/Caltech_DATASAI_Neuroscience_23/blob/main/07_13_23_day4_adapting_preprocessing_data/code/diy_notebooks/colab/DataSAI_Wagenaar_Activity.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQwngwAAsy3B"},"outputs":[],"source":["%pip install  ephysio\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jO2-emRDs-ds"},"outputs":[],"source":["from ephysio import openEphysIO\n","from ephysio import kilosortIO\n","import numpy as np\n","import pickle\n","import json\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLe6p8NRtM9z"},"outputs":[],"source":["root = \"/content/drive/MyDrive/datasai-daw/data/2021-07-20_11-59-01\"\n","oe = openEphysIO.Loader(root)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rp2vQwhKtWxd"},"outputs":[],"source":["oe.spikestreams()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzZ6G9TttYPD"},"outputs":[],"source":["ks = kilosortIO.Reader(oe.contfolder(oe.spikestream(), rec=9) + \"/kilosort20_output\")"]},{"cell_type":"markdown","metadata":{"id":"ZECR8ezX4Byl"},"source":["Frank stores metadata about each recording in JSON files with the experimental data. This is a very convenient way to make sure you don't have to rely on memory for knowing which binary channel in the recording carries which signal:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mI_8YAbJvjMj"},"outputs":[],"source":["with open(f\"{root}/exptINFO_2021-07-20_11-59-01+9.json\") as fd:\n","    info = json.load(fd)\n","info"]},{"cell_type":"markdown","metadata":{"id":"CEvYzi4i4yvf"},"source":["We learn that the sync pulses for electrical stimuli can be found on digital channel 2, whereas the sync pulses for visual stimuli can be found on digital channel 4. We also see that there are two blocks of electrical stimuli, one of type \"boatload\", and one of type \"ramptest\". (Ask Frank to show you the detailed log files to see exactly what that means.) And we see that there were four blocks of visual stimuli. The record shows the names of the python scripts that controlled those stimuli, and the name of \"pickle\" files that store the detailed parameters. We will access those in a little bit."]},{"cell_type":"markdown","metadata":{"id":"8X4fBSns5n7m"},"source":["Let's first extract the time stamps of the state transitions on the digital channels, so we can find out when exactly the stimuli occurred:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXrhdWJVwd_O"},"outputs":[],"source":["evts = oe.nidaqevents(oe.spikestream(), rec=9)"]},{"cell_type":"markdown","metadata":{"id":"Sg3l9imD5yor"},"source":["Do we actually have channels 2 and 4?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YuYZW9M8wtXq"},"outputs":[],"source":["evts.keys()"]},{"cell_type":"markdown","metadata":{"id":"Is9rYaRY598m"},"source":["Yes, we do."]},{"cell_type":"markdown","metadata":{"id":"UBBGgqHn6BUg"},"source":["Let's extract the timestamps of the electrical stimuli:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"132n_tKn7asS"},"outputs":[],"source":["eblks = oe.inferblocks(evts[2], oe.spikestream())\n","len(eblks)"]},{"cell_type":"markdown","metadata":{"id":"GCG6Vm1M6Ngl"},"source":["We have two blocks. Very good. How many stimuli per block?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxQH2Cg77kkG"},"outputs":[],"source":["[len(x) for x in eblks]"]},{"cell_type":"markdown","metadata":{"id":"jUCmpCpl6Shu"},"source":["Looks plausible."]},{"cell_type":"markdown","metadata":{"id":"-da4SMZ66VMH"},"source":["Same for the visual stimuli:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehgMHjnS7o-d"},"outputs":[],"source":["vblks = oe.inferblocks(evts[4], oe.spikestream())\n","len(vblks)"]},{"cell_type":"markdown","metadata":{"id":"VTQXxB6M6Y54"},"source":["Four blocks. Exactly as advertised."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTxkOBTB7xWJ"},"outputs":[],"source":["[len(x.flatten()) for x in vblks]"]},{"cell_type":"markdown","metadata":{"id":"kISXyEQQ6iIp"},"source":["(Why does the counting of visual stimuli look different than of electrical stimuli? The answer is technical: The electrical stimuli are each marked by a short TTL pulse, whereas the visual stimuli are marked by a black-to-white or white-to-black transition of a little square on the corner of the display monitor, which gets picked up by a phototransistor. As a consequence, we only care about the 0-to-1 transitions of the electrical markers, but we care about both the 0-to-1 and the 1-to-0 transitions of the visual markers.)"]},{"cell_type":"markdown","metadata":{"id":"Rpb6GGVo7D5D"},"source":["To get you started, let's focus on the \"gratings\" stimulus block. This compromised multiple presentations of full-screen gratings, with 6 different orientations, 5 different spatial frequencies, and 4 different combinations of eye presentation (explained below)."]},{"cell_type":"markdown","metadata":{"id":"5BNT2d_P7dtd"},"source":["The first step is to extract the stimulus time stamps, and to load the \"pickle\" that contains the relevant stimulus information."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOem6XYs78mE"},"outputs":[],"source":["tframe = vblks[1].flatten()\n","fn = f\"{root}/visual_stimuli/{info['visualstimuli']['1'][1]}\"\n","with open(fn, 'rb') as fd:\n","    pkl = pickle.load(fd)"]},{"cell_type":"markdown","metadata":{"id":"l-l47TUO7oHN"},"source":["So what information do we have?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4LW2Bax8SJ9"},"outputs":[],"source":["pkl.keys()"]},{"cell_type":"markdown","metadata":{"id":"TDxBbZ5p7spa"},"source":["By convention, variables with ALLCAPS names are constants. Let's educate ourselves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KiztWyfw708H"},"outputs":[],"source":["for k, v in pkl.items():\n","    if k==k.upper():\n","      print(f\"{k}: {v}\")"]},{"cell_type":"markdown","metadata":{"id":"rBJ1thFt8Xap"},"source":["What this means, is that the images were presented at a rate of 10 frames per second, 9 repeats of each stimulus, total 120 different kinds of images, 3 s of gray at the beginning of the experiment, 6 orientations (0°, 30°, etc.), four spatial frequencies (0.1 cycles/degree, 0.2 cycles/degree, etc), one phase (i.e., Frank did not in this recording test whether translating the grating by a fraction of the period made a difference), and 4 \"eye\" combinations. These mean, respectively: image presented only to the right eye (1, contralateral to the recording site in left V2), only to the left eye (2, ipsilateral), to both eyes (3), or with a sensory conflict (-1). The sensory conflict meant that the right eye viewed the orientation specified, while the left eye viewed a stimulus that was 90° rotated."]},{"cell_type":"markdown","metadata":{"id":"c8FoQ_7w9p-z"},"source":["How do we know which stimulus was presented when? That's encoded in the \"osp_idx\":"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJHazdK98THL"},"outputs":[],"source":["len(pkl['osp_idx'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twcpvasG8yLX"},"outputs":[],"source":["tstim = []\n","ospe = []\n","for k, x in enumerate(pkl['osp_idx']):\n","    if x:\n","        tstim.append(tframe[k])\n","        ospe.append(x)\n","    # Else, this is a gray frame, which we will ignore for now.\n","tstim = np.array(tstim)\n","ospe = np.array(ospe)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQCFZFEb9E4f"},"outputs":[],"source":["tstim.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUjwylYC9Los"},"outputs":[],"source":["ospe.shape"]},{"cell_type":"markdown","metadata":{"id":"sEaicfX7I5Ev"},"source":["### Exercise\n","\n","Do these numbers make sense? We should have 9 repeats of 6 orientations x 5 spatial frequencies x 4 eye combinations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fx_GeclJFay"},"outputs":[],"source":["# Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"_qQ_G2el-EBq"},"source":["What's in the `ospe`?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2bkalkg9M5L"},"outputs":[],"source":["ospe[:10]"]},{"cell_type":"markdown","metadata":{"id":"pKiwdKwl-Gt6"},"source":["Conclusion, the first column represents the orientation (0 ⇒ 0°, 1 ⇒ 30°, etc), the second the spatial frequency, the fourth the eye (0 ⇒ right, 1 ⇒ left, 2 ⇒ both, 3 ⇒ conflict). The third column is boring."]},{"cell_type":"markdown","metadata":{"id":"5HwsWIkl-YRf"},"source":["### Exercise\n","\n","Look at the first few numbers in the `tstim` vector, and guess whether they represent time stamps in seconds, milliseconds, or samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1m3hMBaZ9N9b"},"outputs":[],"source":["# Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"xx_iu76R-nP8"},"source":["As we saw earlier, KiloSort reports both \"good\" clusters (putative single-neuron clusters) and \"mua\" clusters (multi-unit activity). We will only analyze the former, as the \"mua\"-labeled clusters very poorly matched the other spike sorters, so we we don't trust them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7L5MQbYu9Yrm"},"outputs":[],"source":["tspk = ks.spikesbycluster('good')"]},{"cell_type":"markdown","metadata":{"id":"UaNZ6yJw-_LK"},"source":["### Exercise\n","\n","What is the container type of tspk? Why do you think the data are represented like that? What is the contained type? In what unit do you think spike times are stored?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISl3ESrp_KRe"},"outputs":[],"source":["# Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"AelmUMi0_L1C"},"source":["Let's convert all times to seconds, so we don't have to keep track of sampling rates from here on out."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIyGhSuR9bKN"},"outputs":[],"source":["fshz = oe.samplingrate(oe.spikestream())\n","tstim_s = tstim / fshz\n","tspk_s = { k: v / fshz for k, v in tspk.items() }"]},{"cell_type":"markdown","metadata":{"id":"jjVlk-2e_YEb"},"source":["You can absolutely write your own code to align spike times to stimulus times and count per-stimulus responses, but there are many libraries that provide this functionality, so there is probably no need to reinvent the wheel. If you already have a favorite library, by all means use it. Otherwise, here is mine:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrqKJuqTUJTS"},"outputs":[],"source":["from ephysio.spikestats import SpikeStats"]},{"cell_type":"markdown","metadata":{"id":"a0t4FW9GAKoy"},"source":["We feed it our stimulus times and dictionary of spike times:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPEWyIbLUosM"},"outputs":[],"source":["ss = SpikeStats(tstim_s, tspk_s)"]},{"cell_type":"markdown","metadata":{"id":"lkkJaS2uAO10"},"source":["And now we can extract the latencies of any particular neuron's spikes relative to the stimuli. For example, let's look when neuron #1 fired relative to our stimuli:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K74RSVGqUuPi"},"outputs":[],"source":["lat, tri = ss.latencies(1, -100, 100)\n","plt.plot(lat, tri, '.')"]},{"cell_type":"markdown","metadata":{"id":"K9YceF9yAnlQ"},"source":["### Exercise: Does this look right?\n","\n","Why did we plot time from -100 ms to +100 ms? Why do you think there are fewer spikes during the last 200 stimuli compared to the first 200? What could explain that there are more spikes between -100 ms and -75 ms than between -50 ms and -25 ms?\n","\n","Look at a couple other units as well, to get a better feel for the data. (Remember: don't walk around with a blindfold on.) What else would you like to do to explore the raw spike data?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vyFUaINHVk40"},"outputs":[],"source":["# Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"aXhT0UEaCDY_"},"source":["Let's count the number of spikes fired by neuron #1 in the first 100 ms after each stimulus:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3m-2EsbCYQ6l"},"outputs":[],"source":["cnt = ss.spikecounts(1, 0, 100)\n","plt.plot(cnt, '.')"]},{"cell_type":"markdown","metadata":{"id":"OYVn8z9yCaMN"},"source":["That cannot be right. The raster clearly showed plenty of spikes, yet we count nothing."]},{"cell_type":"markdown","metadata":{"id":"MghkoGvDCjLA"},"source":["### Debugging time\n","\n","Let's (for the moment) assume the library code is correct. Did we use it incorrectly?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdjQEoLfCrtV"},"outputs":[],"source":["ss.spikecounts?"]},{"cell_type":"markdown","metadata":{"id":"_wfoLPZRCwdm"},"source":["Looks like we used the function correctly. What else could be wrong? Did we feed broken data into SpikeStats? Let's take a look:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oy6upgZ_ZdK1"},"outputs":[],"source":["plt.plot(tstim_s, '.')"]},{"cell_type":"markdown","metadata":{"id":"NqSvHo4iC-hR"},"source":["The stimulus times look plausible (nicely sequential, correct number, spanning a reasonable amount of time).\n","\n","What about the spike times?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1e8-ZWcWnwA"},"outputs":[],"source":["plt.plot(tspk_s[1], '.')"]},{"cell_type":"markdown","metadata":{"id":"tfps4madDK29"},"source":["Those are not in order. Is that a problem?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gbudIZDDR6c"},"outputs":[],"source":["SpikeStats?"]},{"cell_type":"markdown","metadata":{"id":"TBFiwVz_DOgK"},"source":["It doesn't say. That's what you get for using homebrew libraries. But it's a good guess. Let's try sorting the spike times before feeding them in."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voeyPaUgDgyi"},"outputs":[],"source":["tspk_s = { k: np.sort(v) / fshz for k, v in tspk.items() }\n","ss = SpikeStats(tstim_s, tspk_s)"]},{"cell_type":"markdown","metadata":{"id":"16T-fk2kDrbX"},"source":["### Exercise\n","\n","Did that solve the problem?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-_KysCADxOf"},"outputs":[],"source":["# Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"omNcPgXEB6dO"},"source":["OK, it did. So now we can extract the responses for each of the cells. For instance:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybAIXwe5yWDF"},"outputs":[],"source":["cnts = {}\n","for celid in tspk_s.keys():\n","  cnts[celid] = ss.spikecounts(celid, 0, 100)\n","len(cnts), type(cnts[1]), cnts[1].shape"]},{"cell_type":"markdown","metadata":{"id":"QXn-FDavy0cy"},"source":["So now we have 1080 response counts for each of 228 cells, stored in a dictionary of numpy arrays."]},{"cell_type":"markdown","metadata":{"id":"JmPUO8kqzCZV"},"source":["## Reshaping the data for further processing\n","\n","For the sake of this exercise, we will feed these data into PCA to see whether the population differentiated between the various gratings we presented to the animal. Let's construct a PCA instance and remind ourselves what shape of data it wants."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eP7qSuOXzjHd"},"outputs":[],"source":["from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bACgARrzzmUA"},"outputs":[],"source":["pca = PCA(10)\n","pca.fit?"]},{"cell_type":"markdown","metadata":{"id":"mRcduRzSzryV"},"source":["So we need to present the data as a single array. In our case, the “features” are the neurons, and the “samples” are the stimuli. (I always have to think twice about that, so let's keep in mind that we need to check that we are doing it right.)"]},{"cell_type":"markdown","metadata":{"id":"Ze_KU76K0BRk"},"source":["At the moment, the data are in a dictionary, so we need to repackage them as a single big array."]},{"cell_type":"markdown","metadata":{"id":"LTnINDa50Ml_"},"source":["### Exercise\n","\n","Extract a vector of cell IDs from the dictionary and a CxK array of the K stimulus responses of each of the C cells."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exmI_HgR0cpM"},"outputs":[],"source":["# Insert your code here\n","cellids = ...\n","resps = ..."]},{"cell_type":"markdown","metadata":{"id":"Ik3r3WKO0b56"},"source":["Now, we can feed the responses straight into the PCA:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwfqUfGtXRjn"},"outputs":[],"source":["pc = pca.fit_transform(resps)"]},{"cell_type":"markdown","metadata":{"id":"_bX4IyIJEO9z"},"source":["### Exercise\n","\n","What should the shape of the `pc` output be? Did we code that right? If not, can you fix it?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTLqBuVYaIfi"},"outputs":[],"source":["### Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"fsHal_gKEnra"},"source":["Let's see if orientation is captured in the first two principal components."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gefFUXKPXXDE"},"outputs":[],"source":["plt.figure()\n","for o in range(len(pkl[\"ORIENT\"])):\n","  idx = np.nonzero(ospe[:,0]==o)[0]\n","  plt.plot(pc[idx,0], pc[idx,1], '.')"]},{"cell_type":"markdown","metadata":{"id":"8RP1gVaBE1yf"},"source":["### Exercise\n","\n","What do you think? Anything there?\n","\n","Check out a few other PCs, perhaps?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1v92xhEGJpw"},"outputs":[],"source":["# Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"6e2Vn9YLiooF"},"source":["### Exercise\n","\n","Is it a problem that we included the conflict stimuli? How can we avoid plotting those? Should we investigate left/right/both eye stimuli separately?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgddhWZtiqku"},"outputs":[],"source":["# Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"kADW5Q47E-JM"},"source":["### Exercise\n","\n","Make a similar plot, but use color to represent the different spatial frequencies.\n","\n","*Hint:* Loop `for s in range len(pkl[\"SPFREQ\"])`\n","\n","In *this* case, are conflict stimuli equally problematic?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQKBpAJ8Xzoo"},"outputs":[],"source":["# Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"9iACjvxwGOmL"},"source":["Do you agree that the population as a whole appears to respond differently to different spatial frequencies?"]},{"cell_type":"markdown","metadata":{"id":"3IWiThTQIFPW"},"source":["### Exercise\n","\n","Make a simple bar plot of total population response as a function of spatial frequency. Do the results mesh with what the PCA indicated?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkFwvVYnIbGj"},"outputs":[],"source":["# Insert your code here\n"]},{"cell_type":"markdown","metadata":{"id":"dY-WdP_rHpzX"},"source":["## The end is the beginning\n","\n","From here, you could explore many follow-up ideas. For instance:\n","\n","* Are there subpopulations that respond strongly to spatial frequencies that the area as a whole responds only weakly to? Nonnegative matrix factorization might be a starting point for exploring that.\n","\n","* Are the responsive neurons concentrated in certain layers of the cortex? You can ask, for any unit, how far from the tip of the probe it sits with `ks.tipdist_unit`.\n","\n","* Do different subpopulations respond more strongly to stimuli presented to one or the other eye?\n","\n","* What are *you* curious about?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qM4illAAHDAH"},"outputs":[],"source":["# Insert your code here...\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMSkH4QxqMZvE4PmzZF90Qc","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
